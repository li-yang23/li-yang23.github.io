---
title: 大模型与图学习的结合场景调研
description: notes for Large Language Models on Graphs：A Comprehensive Survey
tags:
  - machine-unlearning
  - Survey
  - llm
category: AI MachineUnlearning
date: 2024-08-15
---
主要关注大模型和GNN结合的场景，包括与大模型有关的图的场景和大模型在图学习技术上起到的角色。
## 与大模型有关的图的场景
**完全没有文本信息的图**：这种图数据没有文本信息或者没有语义丰富的文本信息，比如交通网络，能量传输网络等。这种图一般作为测试大模型的图推理能力的上下文，或者作为强化大模型的知识源（缓解幻觉）。
**文本属性图**：这种图的节点或者边上有语义丰富的文本信息，也被称为富文本网络，文本网络或者边文本网络，包括学术网络，线上经济网络，社交网络，法律案例网络等。这些图上的任务一般包括学习节点和边的同时具有结构和文本信息的表征。
**文本伴随图**：这种图一般包括一个为整个图结构定义的文本描述，比如分子网络一般伴随一个标题或者文本特征。这种图上的任务一般关注使用图结构和文本描述来理解图的整体信息。
## 与大模型有关的图学习技术
根据大模型的角色和解决图相关问题的最后组件，可以将与大模型有关的图学习技术分为大模型作为预测器，大模型作为编码器，和大模型作为对齐器。
**大模型作为预测器(LLM as Predictor)**：这种类型的技术将大模型作为最后输出表征或者预测结果的组件，可以通过GNNs进行强化。根据图数据注入大模型的方式可以分为
- 图作为序列：不对大模型结构做更改，使用图token序列作为输入使大模型理解图结构。图token序列可以是图的自然语言描述或者由图编码器输出的隐层表征。
- 图增强的LLM：这种方法修改大模型基础模块的结构，使其能够在结构内部完成文本和图信息的共同编码。
- 图敏感的LLM微调：这种方法不对模型输入和结构做修改，而仅在图的监督下对大模型做微调
**大模型作为编码器(LLM as Encoder)**：这种方法一般在文本属性图上使用。GNN是最终的结果输出组件，而大模型作为开始的文本编码器。大模型首先用于编码节点和边相关联的文本信息，输出的特征向量作为GNNs的输入嵌入进行图结构编码。GNN输出的嵌入作为最终下游任务的节点/边表征。但这种方法会遇到收敛问题，数据稀疏问题和低效问题。
**大模型作为对齐器(LLM as Aligner)**：这种方法使用大模型作为文本编码组件，使用GNN作为图结构编码组件，然后将这两个组件对齐。大模型和GNN结合到一起作为最终的输出组件来解决下游任务。大模型和GNN之间的对齐可以分为：
- 预测对齐：轮流使用一个模型生成的伪标签来训练另一个模型
- 隐空间对齐：使用对比学习技术对齐大模型生成的文本嵌入和GNN生成的图嵌入
