---
title: PCGU论文阅读笔记
description: notes for llm unlearning paper ‘unlearning bias in language models by partitioning gradients’
tags:
  - ai-explanation
  - llm-unlearning
category: AI-safety llm
date: 2024-12-14
---
最近的研究表明，大规模预训练语言模型往往会表现出与种族主义、性别歧视、宗教偏见和一般毒性有关的问题。不幸的是，这些预训练语言模型几乎普遍用于下游任务，而自然语言处理通常用于进行现实世界的预测。因此，在开发过程中尽早消除这些语言模型的偏见对于防止自然语言系统造成的无意伤害越来越重要。为此，我们提出了一种称为分区对比梯度反学习 (PCGU) 的新技术，这是一种用于消除预训练掩码语言模型偏见的灰盒方法。PCGU 旨在仅优化对特定偏见领域贡献最大的权重，通过基于对比句子对的梯度计算一阶近似来实现。我们的实验表明，PCGU 既成本低廉，又似乎特别有效地查明大型预训练 Transformer 中隐性社会偏见的来源。虽然我们仅在性别职业领域使用 PCGU 进行训练，但我们发现这样做也可以部分减轻其他领域的偏见。

## 本文的背景

在过去的几年中，由于大型预训练语言模型的普及，大多数自然语言处理应用都取得了非凡的进步。这些语言模型之所以取得非凡的表现，不仅是因为注意力等机制，还因为从文献和互联网上抓取的丰富多样的自然语言语料库。然而，尽管采取了一些措施来确保这些自然语言句子的质量，但最近的研究表明，预训练语料库包含许多有毒/有偏见的句子，并且基于此类数据训练的神经模型很容易捕获和展示这些偏见。

先前的研究表明，嵌入和模型编码了有害的社会偏见。模型中编码的负面刻板印象和社会偏见可能会导致生产系统的不公平和危害。如果没有有效的缓解技术，使用这些有缺陷的语言表示的微调模型可能会意外地继承不代表现实世界或目标任务的虚假相关性。

## 本文研究的问题

最终理想目标是消除预训练掩码语言模型造成的危害。为了减轻表征危害，本文关注两个不同粒度的目标。第一个目标是消除模型的偏差，使其推理中编码的偏差最少。第二个目标是消除整个模型中的社会偏见，使模型尽量减少可能导致其自身在预测中产生偏差的结构。（**第一个是从输出角度看的，减少模型输出偏见内容的可能性。第二个是从模型自身来看的，调整模型参数让模型完全忘记偏见信息。**）

尽量**降低采用去偏语言模型的成本**是去偏的首要任务，因为任何障碍都可能导致人们对社会效益产生怀疑。我们的目标是尽量**减少表征偏差**，同时**最大限度地提高模型的表征能力**。

也就是说，整体目标是移除模型中的偏见，降低偏见内容的生成能力。具体目标可以拆分成三个：减少表征偏见，保留模型表征能力，降低去偏成本。

## 已有方法为什么不行

当时已有方法包括静态词嵌入，上下文词嵌入，词水平以上的模型去偏，模型微调，以及知识编辑。
+ 静态词嵌入方法的理念是句子中的单词是流经语言模型的所有信息的根源，这些方法通常通过投影到不编码目标偏差的某些子空间来操作。然而，现代语言模型不使用外部嵌入，因此目前尚不清楚此类方法是否可以应用于Transformer。
+ 上下文词嵌入通常不考虑模型在实际句子中使用时不同部分之间的相互作用。相反，它们要么关注（静态）词嵌入层，要么关注特定词的聚合表示。
+ 词水平以上的模型去偏方法大多数旨在改进另一个模型将进一步使用文本编码器生成的句子表征的情况，这并不能解决掩码语言建模这样的单词级别的问题
+ 模型微调方法旨在改变预训练或微调过程以防止语言模型学习偏差，大部分方法是通过更改训练过程或向训练损失函数添加偏差感知项来更改嵌入、分类器或编码器的训练过程。还有一些方法以某种方式改变或增强训练数据，通常是添加高质量的无偏见或反刻板的句子，消除公然有偏见或刻板的句子，或者通过替换训练语料库中的文本来组合两者。其他方法包括使用反事实或者对抗信息来阻止模型编码偏差。
+ 知识编辑方法显式编辑模型中的特定知识而不影响不相关的知识，近期方法包括基于梯度的方法，训练单独的网络来预测有效的梯度更新，以删除或替换模型的知识。但是，本文目标是试图消除这些偏见的普遍形式，而不是消除/改变知识编辑方法试图做的更有针对性和具体的知识

## 本文的方法

本文的方法分三步。首先，必须计算一对对比句子的梯度，其差异在于模型偏向的领域。  接下来，我们应用基于梯度的权重重要性算法来计算最重要的权重排序（即，似乎最能编码希望忘记的偏差的权重）。最后，以早期的梯度和有序权重作为输入，计算偏差梯度的一阶近似，并执行语言模型的标准优化步骤。

**对比梯度**

本文考虑的是掩码语言模型（BERT），根据左右token的内容预测掩码部分的token，因此可以计算指定位置是任意token的概率来探索模型的偏见。为了计算对比梯度，本文使用了Winogender Schemas数据集的一个子集，其中包含240个最小句子对，每对句子的唯一区别就是句子主语的性别代词，句子主语是用职业代表的人，所以可以使用性别代词来判别模型对职业的性别偏见情况。

对于最小对中的每个句子，本文计算模型分配给不同token的概率。然后使用标准反向传播计算相对于模型权重的概率梯度$$\nabla_1,\nabla_2\in\mathbb{R}^d$$。（这里计算的是整个句子的？还是掩码token位的？）

**权重重要性判定**

本文首先对权重进行了分组（文中说是要提升鲁棒性），分组分两种，第一种是input aggregation，就是横着划分权重，每行单独作为一个权重向量；第二种是output aggregation，就是纵向划分权重，每列单独作为一个权重向量。因为算出的梯度是同规模的，因此也用相同的方式划分。

然后是要计算每个权重向量的重要性。本文遵循的假设是模型中的一个权重子集会负责编码全部的偏见。因此本文会期望前面计算出的对比梯度在某些权重组上出现明显的差异，而在其他权重组上没有。因为这个掩码位置是要生成跟性别相关的token，那么负责编码性别偏见的权重的梯度会编码让模型预测对应性别词的概率的最大上升方向。本文使用了余弦距离评估每组权重的重要程度，并据此为权重排序。

$$
Importance(\theta^i)=\frac{\nabla_1^i\cdot\nabla_2^i}{\Vert\nabla_1^i \Vert\Vert\nabla_2^i \Vert}
$$

相关对比梯度块具有低余弦相似度的权重向量被确定为对目标偏差最为重要。相反，相似度高的权重向量被确定为对该偏差最不重要，但可能与不相关的概念或不同类型的偏差更相关。

**一阶梯度优化**

最后在优化部分选择性的优化一部分权重向量，选择排序后前k个最重要的权重向量（就是这一部分权重）进行更新。在选择更新所用的梯度时，采用了数据集的统计信息来判定句子对的优劣。本文使用了性别为女性的新闻句子中的性别-职业共指对的比例。如果使用的数据集里面代词共指比例比这个低，那这个性别就是劣势性别，句子就是劣势句子，对应的另一个句子就是优势句子。

最后本文从原来的权重里面减去优势句子带来的梯度，达到让模型忘记性别偏见的效果。方法是使用梯度下降，用学习率调整削减幅度。

$$
\theta^{r_i}\leftarrow\theta^{r_i}-\alpha\mathbb{1}\{i\leq k\}\nabla_{a_1}^{r_i}
$$

## 本文如何说明效果

在实验中尝试消除性别-职业领域的一组掩码语言模型的偏差，以便它们的最终参数对 MLM 预测的不平等性进行编码。具体来说，我们的目标是更新模型，使它们通常不会偏向刻板印象句子或反刻板印象句子，因为即使是反刻板印象也可能是有害的。本文随后使用现有的评估基准来评估 PCGU 的有效性。

本文在两个社会偏见基准上进行了评估，StereoSet和CrowS Pairs。StereoSet中的每个实例都是一个句子三元组，包含一个有刻板偏见的，一个反刻板偏见的，还有一个无意义的。CrowS Pairs每个实例都是一对略有不同的刻板句子和非刻板句子。

本文使用刻板印象分数（SS），语言建模分数（LMS）和理想化上下文关联测试分数（ICAT）评估刻板偏见程度（StereoSet和CrowS都用了）。
为了衡量模型的语言建模能力，LMS 被提议作为刻板印象/反刻板印象句子被分配的概率高于无意义句子的示例比例。因此，理想模型的得分为 1，去偏方法应旨在在去偏过程中尽量减少该分数。
为了衡量去偏后更好的 SS 和更差的 LMS 之间的权衡，ICAT 将两者合并为 0 到 1 之间的分数，这样完全去偏且准确的模型可以达到 1 分（同样，完全随机的模型可以达到 0.5 分）。

本文测试了Bert系的一堆模型（BERT-110M，ReBERTa-125M，ALBERT-11M），都是huggingface与训练过的。使用了DPCE和AutoDebias方法作为对比方法。

本文发现提出的PCGU方法相较DPCE和AutoDebias方法更加高效，并且去偏结果更好。DPCE 的收敛速度比 PCGU 慢得多，可能是由于对原型的依赖。AutoDebias 性能相对较差的原因可能是它找到分布差异最大的提示的方式。这种启发式方法没有考虑到这样一个事实，即在强 PLM 中分布差异最大的提示通常是那些其上下文需要一个单词版本并且可能与偏见无关的提示。

本文尝试对所有权重执行 PCGU（基本上是对优势句子进行向后优化步骤），并发现尽管该过程通常能够很好地消除语言模型的偏差，但语言建模功能却受到极大限制（类似于 AutoDebias）。这与权重分区版本形成了鲜明对比，权重分区版本对语言建模能力的下降幅度要小得多。这些结果表明，某种形式的分区显然是必要的；并非所有模型权重对偏差的贡献都相同。

输入与输出聚合分区的选择不会明显影响去偏模型的性能。

本文还研究了在 PCGU 中采取优化步骤降低优势句子的概率与增加劣势句子的概率之间的差异。前者可以加快收敛速度​​，尽管后者收敛到类似性能所需的时间并不长。一般来说，性能差异更多地取决于模型选择标准，而不是用于调整的梯度。

使用每个梯度的目标差异有一些有趣的含义。通过降低优势句子的概率，可以更直接地教导模型减少偏差。另一方面，通过**增加劣势句子的概率，反而教导模型对两种形式的偏差相同**（与其他选项相比）。实际上，偏见有多种形式，我们的工作动机是希望消除整个偏见类别，而不仅仅是特定的例子，一对选项不足以代表选项的完整分布（这两种形式的偏见相同不代表对所有同类型偏见都相同了）。因此，有理由相信，降低优势句子的概率应该更适用于一般形式的偏见。

在训练时根据掩码token的logit动态确定优势句子和劣势句子无法达到去偏效果。

尽管PCGU 的调整集仅包含与性别和职业相关的信息，但本文发现此过程也能够（在不同程度上）改变其他域中的偏差量。这表明，也许控制不同偏差域的一些参数/神经元可能重叠，从而导致训练期间出现一些跨域收敛。然而，SS 的差异也可能仅归因于噪声或与偏差无关的因素。此实验的扩展可能能够确定是否可以同时或顺序消除不同偏差域的偏差，比如通过坐标下降。

## 可能的未来方向

这个时候的遗忘其实还没有明显地提到遗忘，还是在行为上进行评估的，只要最后模型不会输出有害偏见信息就行了，但是这里的方法其实已经开始尝试定位然后调整了。相较其他直接把权重归0地方法，这个方法可能更擅长保留模型可用性。

这个方法主要是在掩码语言模型上做的，后面的方法应该主要考虑生成式语言模型了，可以看一下后面工作是怎么做的。