---
title: 隐私保护大模型阅读笔记
description: notes for Privacy Preserving Language Models
tags:
  - machine-unlearning
  - Survey
  - llm
  - privacy preserving
category: AI MachineUnlearning
date: 2024-11-21
---
语言模型是在大量人类语言文本上为单词序列预测任务训练得到的系统，应用于任何人类使用自然语言的场景，比如呼叫中心，医疗应用，移动电话和PC和家庭助手，电子邮件和信息自动补全服务，文档翻译和检索，写作助手等等。典型的语言模型任务包括部分词性 （POS） 标记和解析、光学字符识别 （OCR）、自动语音识别、自然语言生成、情感分析和自然语言推理。基于这些任务的应用程序会大规模处理潜在的私人数据，例如用户查询、敏感文档、电子邮件和私人对话。

语言模型的目标是生成形如自然语言的句子。一个核心的特征是学习**嵌入函数**，将单词和短语映射到高维空间中的向量上，让两个向量的距离反应对应的单词和短语的相似性。嵌入函数会作为编码单词和句子的中介。另一个核心的特征是**生成和表示文本**，主流的两种模型包含进行下个token预测的生成式语言模型和进行句子填空的遮罩式语言模型。

训练语言模型的方法显示使用超大数据集训练上百亿参数的特大模型可以得到空前优秀的性能。使用大模型，大数据集和大量的计算时间都是获得高性能的关键，实验结果说明基于transformer的语言模型的测试损失遵循一个与模型规模、数据集规模和训练时间相关的幂律关系。

机器学习模型会从训练数据中提取可泛化的模式，但有工作发现记住训练数据中的某些部分是在长尾分布上进行最优泛化的必要条件。数据记忆会直接导致模型训练集中的私人信息泄露，因为模型对训练集中存在的样本的行为变得与不存在的样本明显不同。典型的数据泄露的风险包括：
- 成员推理：成员推理攻击旨在发现某个给定的数据节点是否存在于模型的训练数据中。这种攻击可以作为隐私风险分析工具，发现模型能够记住多少训练数据中的个体样本，以及个人用户的风险程度。一个无法接触模型和训练过程的攻击者也可以通过查询模型来鉴定训练数据的成员。
- 训练数据提取：训练数据提取通过与模型交互发现部分训练数据样本，成员推理攻击可以作为预言机来判断生成的样本是不是训练样本。机器学习模型面临数据隐私的高维诅咒：模型的规模越大，能记住的训练样本就越多，就更容易遭受训练数据提取攻击。
- 属性推理攻击：属性推理攻击旨在从部分已知的训练记录中推断出有关个人的敏感属性，它假设攻击者可以接触到数据记录中不敏感的属性以及模型的输出。
攻击方法会随着时间进化，因此目前的攻击结果无法全面描述语言模型的隐私风险。因此需要设计隐私保护的语言模型，即让模型能够学习人类语言的整体分布和结构，同时不会记住隐私信息。目前构建隐私保护语言模型的方法主要包括四类：**数据清理技术**在文本中查找隐私信息片段，并在任何进一步处理之前将其删除；**差分隐私技术**训练模型来缓解记忆风险；**机器遗忘技术**用于从模型中去除隐私信息的影响；**模型编辑技术**用于修改模型中保存的知识从而保护隐私信息。
## 大模型隐私保护的含义和难点
如果声称一个语言模型能够保护隐私，那么这个模型必须**仅能够**在正确的上下文中向正确的人展示隐私信息（“秘密”），其中包含三层意思：什么上下文中可以共享秘密而不侵犯隐私；秘密中可以包含什么信息；哪些人可以知道秘密（“in-group”）。根据这三个维度，隐私信息可以分成以下类别，只有充分理解上下文并遵循隐私规范，我们才能构建完全保护隐私的语言模型。

| 固定格式        | 拥有者  | 组内规模       | 组内共享             | 例子                                      |
| ----------- | ---- | ---------- | ---------------- | --------------------------------------- |
| $$\bullet$$ | 1    | 1          | -                | 个人密码文件，秘密文件                             |
| $$\bullet$$ | 1    | >1         | $$\bullet$$      | SSN，密码，发给别人的卡号                          |
| $$\bullet$$ | 1    | $$\infty$$ | $$\circleddash$$ | 发布在github上的联系方式，个人信息公开发布在网络上，但有明确定义的上下文 |
| $$\bullet$$ | >100 | >100       | $$\bullet$$      | 在员工之间共享的公司信用卡号                          |
| $$\circ$$   | 1    | 1          | -                | 个人搜索历史                                  |
| $$\circ$$   | 1    | 2          | $$\bullet$$      | 和医生的咨询记录，在有危险情况前都不应当告诉别人                |
| $$\circ$$   | 1    | 3          | $$\bullet$$      | 公司员工为了保险登记共享公司以外家人的身份证号                 |
| $$\circ$$   | 1-2  | >1         | $$\circ$$        | 朋友共享了另一个朋友的离婚消息                         |
| $$\circ$$   | >100 | >100       | $$\bullet$$      | 论文在发布前被300个人内部讨论过                       |

> 也就是上面说的，一个信息是否是隐私信息与其所处的上下文环境，描述的对象和被访问的对象相关。在语言模型中，隐私的泄露可能根本不需要本人的知情，不需要主体进行泄露。比如公司员工登记家人的身份证号，比如共享的八卦信息，都是在本文授权和知情之外出现的泄露。

整体而言，语言模型中的隐私包含四个特点：上下文相关，难以定义，传播范围难确定，保护方式难以仿生。
### 隐私是上下文相关的
人类并不是简单地记住哪些信息是隐私的，哪些信息不是隐私的，而是根据对话和社会文化背景对适当性进行复杂判断来保持信息的私密性。在决定是否在训练中使用数据时，必须考虑用户要在其中共享数据的范围。

一个信息是否隐私并不是简单的二元变量，一个人的隐私很可能对另一个人而言是可共享的。更广泛地说，数据可以位于具有不同限制和期望的一系列隐私级别上，介于完全公开（例如维基百科）或完全私密（例如某人的搜索历史）两个极端之间。

用不属于该公开级别的数据训练供公众使用的语言模型会违反该数据的原始隐私期望。Nissenbaum的上下文完整性提供了一个框架，用于明确哪些上下文信息可以在谁下共享以及与谁共享。在上下文完整性框架中有五个特征（数据主体，发送者，接收者，信息类型和传输原则）来控制一个信息的隐私性，任一特征的修改都会导致信息的隐私期望发生变化。*（我理解）这五个特征构成了隐私信息的上下文（“情境”），如果信息在上下文外共享了，就视作信息的隐私性被违反了。* 

语言模型难以充分理解上下文，从而准确地判断信息的隐私性。相关的工作包括对于敏感场景下的恰当反应，比如性骚扰，自杀话题等其他心理健康话题。另一种就是上下文敏感，长形式，道德和个性基础的回答生成，比如聊天机器人或对话系统应该在考虑到先前上下文的情况下进行对话。另外，不同的上下文中的语言内容可能包含相同的隐私信息（比如第三方转述或者八卦），识别其他上下文中的隐私信息更有挑战。
### 隐私难以鉴定识别
隐私信息可以有多种形式和含义，可以有多种形式来交流信息。即使电话号码这样具有预先制定的格式的信息也可能有多种表现形式。识别不同形式的隐私信息本身就是很有挑战的。没有固定格式的隐私信息就更加难以识别，为语言模型注入足够的社会背景和意识来识别这些联系似乎具有挑战性。

一次又一次地陈述相同的隐私信息并不会降低其隐私性。也就是说不能简单根据信息在训练集中出现的次数，或者大模型生成信息的置信程度来判断是否是隐私信息。在这种情况下，尊重隐私需要能够识别私人信息并收集引用此私人信息的所有训练数据项。

语言是进化的，隐私信息也在不断进化，语言或社会规范的变化可以**改变人们使用语言表示秘密（或某事是否被视为秘密）的方式**。**信息的隐私性是随着时间在变化的**，有些信息在过去不算隐私，但现在算了。比如论文在审核的时候就是具有隐私性的，但审核通过公开发表之后隐私性就会消失。一个要自动检测敏感文本片段的模型必须意识到语言含义的变化。**这种变化可能是很快的**，比如社交网络中的某些概念可能随着某些社会运动发生含义的突变。此外，为了规避针对特定关键字和短语的自动审查和过滤方法，特定主题通常会**用新单词和短语重新表示**。尽管语言和隐私会自然发展，但语言模型通常只在静态数据集上训练一次。随着时间的推移，这些数据集以及在其上训练的语言模型对于理解当前语言的用处会越来越小。
> 所以这里存在对于大模型的遗忘和持续学习的问题，通过遗忘和持续学习修改信息的隐私性。
### 内团体（in-groups）难以识别
是否与给定个人共享隐私的决定因隐私而异，因此在不同上下文中每个隐私的内团体是不同的。许多数据隐私方法（尤其是差分隐私）都隐含或明确地假设用户的私人信息不会超越用户自己的数据（即，用户只需不共享自己的数据即可保护自己的隐私）。但许多隐私信息会被内团体的其他人传播，定义一条隐私信息的唯一“所有者”可能很困难。此外，隐私信息还有可能被团体外的人传播，比如社交媒体耳语网络几乎完全致力于分享有关不在网络中的人的私人信息。某些公司还有可能为没有账号的用户创建影子账户，在没有任何个人贡献的数据前，公司依然可以收集足够对个体进行分类的数据。在通常用于训练大型语言模型的 Web 抓取数据集中，通常不可能明确地将单个信息片段映射到特定的 “所有者”。

内团体本身也没有明确的上限范围。对于任何个人秘密，我们可以尝试识别知道该秘密的组内人员，从而尝试从整个群组中抹除所有对这个隐私信息的描述。一种简单的直觉是多次提及的信息没有那么隐私。但这种直觉不适用于某些隐私信息，并且没有一个明确的数字$k$可以划定敏感与非敏感的用户规模。一些信息对团体外的人无疑是隐私的，但这些数据可以分享给团体内的大量成员。导致无法有效地识别所有团体内人员，从而无法有效抹除所有对这个隐私信息的描述。
### 隐私的人类观念
机器学习的隐私保护机制关注于让模型不要记住隐私信息，但人类不同。人类明确知道学到的敏感信息，但人类使用习得的对话规则来衡量在给定上下文中分享某事的适当程度或礼貌程度。比如格莱斯的合作原则、礼貌理论、相关理论等交流框架。这些原则容易描述，但是非常依赖上下文，导致难以用技术处理。这些框架需要事先了解参与对话的人、社会文化背景和过去的对话。在仅有对话文本的情况下，可能无法获得充足的上下文信息来判断生成的信息的隐私性。已有心理学研究说明我们最有可能记住的信息要么与我们以前看到的非常一致，要么与我们以前看到的非常不同。总而言之，我们尊重自然语言中的隐私，不是因为没有记住秘密，而是通过判断任何给定的信息是否适合在给定的上下文中与给定的一方共享（除非我们要错误地或恶意地共享它）。
> 我理解就是你要么让模型直接不要记住这是隐私信息，要么让它知道是隐私信息，然后在生成的时候避免生成这样的隐私信息。前一种方法就是知识遗忘，后一种方法就是偏好对齐。

### 大模型隐私泄露的威胁模型
整个大模型的训练部署流程是预训练$$\rightarrow$$继续预训练$$\rightarrow$$监督微调$$\rightarrow$$强化对齐。最后得到的大模型一般会通过白盒或者黑盒方式发布。白盒方式包括在官方网站或者huggingface这样的第三方网站发布模型参数和配置文件，用户可以直接下载文件并访问模型中的所有数据。黑盒方式包括提供API等方式，用户仅可以接触模型的输入和输出数据，可能包括输出的内容，文本嵌入表征，以及预测的概率分布。

在威胁模型中一般包括两个参与方：拥有敏感数据和大模型的防御方，以及通过访问防御方提供的服务来泄漏敏感信息的攻击方。在白盒场景下，攻击者可以访问大模型的所有组件，包括权重、梯度、结构等。这种场景的例子包括：
- 攻击者是一个组织内部的内部人员，可以访问训练好的模型
- 整个模型是公开的，并且训练数据的分布是已知的
- 由于每个用户和服务器都观测模型参数，联邦学习可以成为白盒环境下的真实世界例子

在黑盒场景下攻击者只能访问模型的输出，包括任意序列的概率向量，从而得到损失值。但是攻击者不能得到模型的权重或梯度信息。

攻击者的目标包括 
- 泄露D中数据点的信息，例如推断敏感属性或特定用户生成的特定文本数据的缺失/存在；
- 重建用户提交给D的文本序列的逐字稿

攻击的手段可以分为
- 成员推断攻击：旨在推断是否使用特定的数据点来训练模型；
- 属性推断攻击：旨在推断训练数据的敏感属性；
- 数据提取攻击：旨在从训练数据中重建逐字节的文本序列
#### 成员推理攻击
成员推理攻击的目的是判断给定的数据样本是否存在于训练数据中，基本的思想是通过计算成员分数$$f(x;M)$$是否超过某个特定的阈值$$t$$来判断一段给定的文本样例是否是大模型$$M$$的训练数据的一部分。可以通过成员分数的类型将已有的推理攻击方式分为八种
> TODO: 根据可访问的信息，依赖的手段将这八种做一下梳理
- reference-free loss-based
- reference-based loss-based
- likelihood ratio attacks
- zlib entropy attack
- min-$$k\%$$ prob
- data perturbation
- model perturbation
- paraphrasing-based
> 但这种攻击是不是不太管隐私性啊，根据前面说的隐私保护是要在指定的上下文中给指定的人展示指定的内容，这个东西好像不太管前面两个指定，只看有没有指定的内容

目前的部分分析结果包括：
推理攻击方式可以分为基于扰动的方法和基于损失的方法。给予扰动的方法在可以获得llm的pythia工具的时候强于基于损失的方法，两种方法的结合可以提升成功率。也就是说基于扰动的方法需要在一定程度上获得白盒权限，但不需要像基于损失的方法那样训练reference模型，而且不需要接触原来的训练数据分布中的数据，它们可以使用其他模型通过随机替换部分token的方式来扰动输入。

更大规模的模型容易被成员推理攻击方法攻击，因为更大的模型有更多的参数，因此更容易过拟合并记住训练数据。基于decoder的gpt系列模型相较基于encoder的bert系列模型更容易遭受攻击，但可能是因为更大的模型规模。

考虑微调的影响时，我们考虑两种场景：第一种是攻击者对预先训练的模型和微调的模型都有黑盒访问权限；第二种是攻击者只能对微调模型进行黑盒访问。攻击者的目标都是确定预训练模型的训练数据成员。第一种场景里面攻击者可以使用预训练模型和微调模型之间的概率差异作为成员分数，第二种场景里面可以根据向微调模型的影子模型的查询结果构建一个预测向量数据集，以训练可以推断隶属度的元分类器。微调最后一层对成员推理攻击最敏感，说明大量参数，并且位于模型末尾的参数具有很高的记忆微调数据的能力。

在训练数据未知时，为了度量成员推理攻击的成功率，可以通过一些侧信道信息，比如模型训练的时间，来整理数据，并将在模型训练当年之前发布的数据视为训练数据的成员，并将之后的数据视为非成员。使用reference model可以有效提升对预训练模型的攻击成功率，使用与受到攻击的 LLM 相同的架构，以及与目标数据集更相似的参考数据集可以提升reference model的质量，从而提升攻击成功率。

上下文学习ICL为增强的数据带来了隐私风险，相较微调的 LLM 更容易受到 MIA 的影响。由于预训练 LLM 的两个典型特征，预训练LLM中MIA的威胁较低：训练数据量大（通常 LLM 使用数十亿到数万亿个令牌进行训练）和训练 epoch 数量少（由于 dataset 大小大及其过拟合倾向，通常 LLM 训练一个 epoch）。相较预训练模型，微调模型使用的数据量少，训练批次多，更容易受到成员推理攻击的攻击

MIA 的 AUC 分数随着文本长度的增加而增加，可能因为较长的文本包含目标模型记住的更多独特信息，使其与看不见的文本更容易区分。随着训练集大小的增加，MIA 的成功率单调降低。可能说明训练集中的样本数量有助于减少过拟合并降低 MIA 的 AUC 分数。MIA 性能会达到峰值，然后随着看到的训练数据量的增加而逐渐下降。这可能因为在训练早期，数据与参数计数的比率较小，并且模型可能倾向于过度拟合，但随着训练的进行，模型会更好地泛化，从而降低攻击能力。数据训练顺序和数据重复对于MIA成功率的影响目前还不清楚。
#### 属性推理攻击
属性推理攻击旨在从部分已知的训练记录中推断出有关个人的敏感属性。属性推理攻击假定攻击者拥有有关数据记录的非敏感属性的信息，并且有权访问经过训练的模型的输出。具体而言，如果一个文本一个缺失的属性有多种可能值，攻击者会构建多个候选文本，然后输入模型，根据困惑度来选择最有可能是训练数据的那个。一般会包括训练一个单独的**属性推理模型**，使用目标模型对属性值的嵌入结果作为输入来预测是否是正确的属性。另一类方法会使用公开数据中属于目标的数据，将其放入**特定的模板**中，要求大模型辨别作者的个人特征。实验结果说明LLM可以从不同来源的非结构化文本推断个人作者属性的集合。虽然人类也可以手动收集并推理，但大模型可以完整更加快速和精准的信息收集。
#### 数据提取攻击
数据提取攻击旨在从语言模型中提取记忆的训练文本实例，因为训练数据可能包含电话号码，地址等信息，提取结果很有可能侵犯他人隐私。数据提取攻击可以分为**有目标提取**和**无目标提取**两类，区别在于攻击者是否准备了搜索恢复的文本实例。无目标提取关注找到任意记住的文本实例，而有目标提取旨在找到给定前缀的后缀，从而提取指定的信息。

数据提取攻击一般包含三个步骤，首先是**前缀生成**，一般是无目标提取使用，首先根据爬虫生成一些前缀避免无意义输入，也可以初始化为空；然后是**后缀生成**，将前缀输入模型，让模型生成后缀，一般会修改生成时的参数，促使模型生成多样化的结果，以尽可能诱使模型输出记忆结果；最后是**文本选择**，生成的后缀按可能性顺序进行重复数据删除和排序，并使用成员推理攻击 （MIA） 保留最有可能记住的文本实例。

数据提取的有效性常常归因于训练数据中的重复序列。在所有方法中，攻击性能随着前缀长度呈指数增长。随着预测后缀长度的增加，攻击性能下降。可提取序列的比例随着前缀token的数量呈对数线性增加。

在Encoder模型上，目前数据提取攻击只能成功重建两个标记，因为此后搜索空间变得太大。如果算法生成更多候选者（即更大的光束尺寸），重复仍然会重新出现丢失的标记。高重复率可以提高提取的成功率，这与重复会加剧记忆的研究结果一致。
> 看大模型物理学的时候发现的，感觉不能说是重复，应该说是大模型见过对应信息的次数。

### 大模型隐私泄露的防御方法
对于大模型隐私泄露的防御方法分别位于训练和部署阶段，可以分为数据预处理，隐私保护训练和遗忘方法。数据预处理方法一般在预训练阶段之前，隐私保护训练方法分布于预训练，微调，压缩，联邦学习和部署的上下文学习阶段。遗忘方法可以分布于多个阶段中。

#### 数据预处理方法
数据预处理方法包括数据清理和去重。数据清理旨在在模型训练之前从数据中删除所有敏感信息，广泛应用于从医疗文本数据中删除个人识别信息（PII）。但在自然语言中形式化定义私有信息本质上是复杂的，这使得设计一种自动清理方法来保证删除所有潜在敏感序列具有挑战性。当敏感信息遵循独立于上下文的一致格式时，数据清理方法非常实用。数据清理在保护隐私方面的有效性无法精确衡量或保证，这表明它不应作为文本数据的唯一隐私保护措施。

训练数据去重旨在从训练文本中删除重复出现的文本序列。使用基于后缀数组的算法从训练数据中删除重复序列，可以让GPT-2重新生成的训练数据减少大约 10 倍。实验说明了GPT-Neo具有从重复训练数据的训练数据中生成精确序列的可能性。从训练数据中删除完全相同的序列还可以保护 GPT-2 免受提取攻击，而不会降低模型性能。在去重的训练数据上训练的Decoder-based LLM明显不易受MIA影响。但是，通过编辑距离测量的近似记忆力仍然很高。重复数据删除有效地防止了 LLM 的训练数据泄漏，但侧重于平均减少记忆，并且不能保证防止特定训练示例的记忆。

#### 隐私保护训练
差分隐私训练方法提供了隐私保护的形式化保证。如果算法的输出在仅相差一条记录的相邻数据集上无法区分，则该算法为DP。DP随机梯度下降方法在DP的框架下训练ML模型，为普通的SGD引入两个改动：（1） 将单个样本的梯度裁剪为固定范数C，以限制单个训练样本对模型更新的影响；（2）将校准的高斯噪声（与 C 成正比）添加到聚合的裁剪梯度中，然后利用该噪声梯度来更新模型参数。

联邦学习避免直接交换隐私数据，模型在本地客户端上更新，客户端仅将模型权重发送到中心服务器上。但是，FL 期间的模型更新（包括梯度和参数）可被故意利用来揭示敏感信息。隐私可以将FL与DP结合来提供有保障的隐私保护。将 DPFL应用于LLM 的挑战包括 （1）LLM的大量参数会导致巨大的有效载荷，从而给服务器带来通信压力;（2）词汇量大意味着自然语言中词频的分布表现出明显的偏度和长尾分布，因此生僻词的信噪比非常小，导致 DP 训练过程中的信息丢失。

在DP-SGD中计算每个样本的梯度会产生大量的内存和计算开销，这给参数数量和词汇量较大的LLM带来了重大挑战。因此一个重要挑战是提高DP训练的效率，同时保持准确性。在训练期间将batch size扩展到数百万（兆级）并使用递增的batch size计划在DP-SGD下实现了更高的准确性和更快的预训练。但准确度在非隐私保护的标准上训练的结果会下降最多$$10\%$$左右。

DP隐私分词（tokenization）是另一种隐私保护训练方法，这包括将噪声注入语料库的单词直方图，DP 训练的一些效用下降可以通过使用DP分词器来恢复。

另一路方法关注在微调阶段使用DP-SGD，主要是通过使用参数高效微调来提升训练效率。在Bert等方法上的结果显示将其与精心选择的超参数（包括更高的学习率和更小的裁剪限制）相结合，甚至可以在具有严格隐私保证的下游任务上胜过非隐私微调的基线。较小的微调LM性能在DP训练中比较大的LLM下降得更快。

差分隐私主要应用于预训练和微调阶段。在预训练阶段使用DP不会明显影响在下游任务上的表现，但目前还没有对差分隐私预训练和微调之间的交互的研究。还有工作研究使用DP来压缩模型，通过使用 DP-SGD 训练教师模型，然后执行知识蒸馏，从而产生 DP 学生模型。

使用 DP 进行训练时，在微调训练语料库上训练的模型性能下降更多。因为微调数据一般规模更小且包含更多敏感信息，在这些数据上平衡可用性和隐私性会更加困难。此外，一个问题是在 DPSGD 期间，噪声被均匀地添加到所有数据点中，这意味着对于数据中的稀有组，收敛将更加困难。如果数据并不均衡，均匀地添加DP噪声可能导致在稀有数据上出现更加明显的表现下滑。解决偏倚的技术（例如反事实数据增强）可以与 DP 相结合，以帮助减轻这种影响。
#### 隐私保护推理
隐私保护推理旨在保护训练数据中的私人信息在推理时不被泄露到模型输出中。可以将DP应用于推理，使得对于对抗性选择的特征，无论某个单个数据点是否在训练集中，模型的预测都不会有太大变化。

一种方法使用对隐私数据的不相交部分进行微调的LLM集合。在推理时，如果所有模型都同意 next-token 分配，这直观地意味着 next token 不可能从训练数据中泄露敏感信息。但是如果观察到模型之间存在高度不一致，则预测将与公共预训练模型（微调模型初始化的模型）的预测混合，以最大限度地减少隐私泄露。但是从 LLM 集合进行解码可能会产生不连贯的文本，并且会成倍增加存储和计算量。

另一种轻量化方法是解码阶段训练模型的扰动机制，扰动输出分布是通过在给定输入的原始模型输出分布和均匀分布之间进行线性插值来获得的。扰动概率可用于从词汇表中随机选择一个标记来填充掩码。但是在RoBERTa-style上训练的结果出现了性能的明显下滑。
#### 机器遗忘
机器遗忘要求从模型中删除与个人关联的数据的影响。精确遗忘要求在一个不包含敏感数据的数据集上重新训练模型。近似遗忘通过生成一组新的权重来近似于重新训练的权重，从而从训练模型的权重中消除特定训练示例的影响。

一种方法是最小化负对数似然，在语言建模期间反转训练目标以忘记原始训练数据中的特定示例（“忘记集”）。*遗忘的成功是通过测量一个点的记忆准确性和对提取攻击的敏感性来衡量的。一个样本的记忆分数如果低于1000个未见过的样本的平均记忆分数，就视为这个样本被遗忘了*。但这种方法依赖于选择的样本， 并且降低了生成后缀的流畅性和连贯性，特别是对于较大的 GPT-Neo 125M，这严重降低了下游分类和对话任务的性能。较大的 LM 需要较少的 epoch 来忘记序列，这表明较大的 LM 是更强的“取消学习器”。在较大的样本量（最多 128 个）下遗忘会显著降低模型性能。

另一种方法是hi使用强化学习反馈循环来遗忘序列。给定前缀和后缀，前缀输入大模型来生成后缀。然后计算生成的后缀和真实后缀的相似程度作为奖励信号来微调模型，从而鼓励遗忘。较大的模型往往会更快地忘记记忆的数据。将此方法与重复数据删除相结合可进一步增强隐私，证明了结合隐私保护方法的有效性。在某些例子中观察到较高的困惑度（较低的效用），这可能是由于仅针对序列的差异性进行了优化，而不是针对困惑性进行了优化。多目标强化学习可能可以提供一个候选改进方案。
#### 模型编辑
模型编辑方法旨在通过更新、擦除或插入知识来更新模型与特定编辑描述符有关的行为。

基于Transformer 架构中的前馈网络模块可以比作键值内存，其中每个键表示一个文本模式，每个值表示词汇表中的分布的观察，locate-then-edit 技术建议识别网络中哪些参数存储特定知识，并修改它们以编辑这些参数。来自训练数据的隐私信息可以存储在特定的神经元中，这代表着可以通过检测并删除这些隐私神经元来移除隐私信息。

为了评估隐私信息的隐私归因分数，可以使用梯度积分来计算多个标记同时对神经元激活的贡献。具有最高归因分数的前$$z$$神经元可以将其激活值设置为零，以擦除相应私人信息的模型记忆。总体而言，目标是在给定上下文的情况下降低包含隐私信息的序列隐私泄露的可能性。在BERTbase上的结果发现泄露风险依然高于DP微调，但计算成本明显低于 DP 训练。但是，需要编辑更多的神经元才能更好地擦除数据，这会导致模型性能显着下降，并且还会降低隐私风险的有效性。

另一种方法是对已训练的LLM进行最低限度的微调，以在查询特定的知识域的时候生成安全的响应。可以通过在问答对上微调模型，并将敏感答案替换为数据中的安全响应。使用 LoRA 仅微调 MLP 层中的权重矩阵就足够了。使用提取攻击的定性评估表明，该方法降低了提取攻击风险。但是，如果广泛应用，由于严格的过滤，它可能会降低实际应用中的实用性。