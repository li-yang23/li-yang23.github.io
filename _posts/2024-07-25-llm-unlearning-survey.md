---
title: 大模型遗忘综述论文阅读思考笔记
description: notes for Digital Forgetting in Large Language Models-a survey of unlearning methods
tags:
  - machine-unlearning
  - Survey
category: AI MachineUnlearning
date: 2024-07-25
---
## 遗忘的动机

遗忘的需求主要来自道德原则，国家和国际条例，以及行为准则。通常来说，与个人隐私相关的保护条例（GDPR）以及对知识产权的保护驱动着对于包括大模型在内的软件系统的遗忘的研究。
### 隐私保护
在预训练时，大模型需要使用大量从网络上获取的数据进行训练，错误配置的网络服务可以包括个人或组织内部的私人数据，这些数据可以被搜索引擎索引并自由访问。这些非预期公开的数据可能最终会出现在用于LLM预训练的训练数据集中。在后续的微调中，隐私数据可能会被用来微调预训练模型使其适应下游任务。训练数据中的数据很容易被记住然后被泄露出来[^Shokri2017][^Salem2018]，其中的边缘数据更是如此[^smith2023]，而这些出现次数很少的边缘数据很可能与个人信息相关（就是因为只与某些人相关，所以才在边缘）。

针对机器学习模型的最基础的攻击方法就是成员推理攻击，攻击者会尝试判断一个给定的数据节点是否包含在训练数据中。本质而言，成员推理攻击是要找到模型在见到训练数据前后的表现分别是什么，表现的区别可以使用损失，分类置信度或其他标准来衡量。对于大模型而言，一个常见指标是迷惑度，即模型对于一个给定的文本序列的确定程度。

原来的工作一般认为针对模型推理攻击的脆弱性与模型的过拟合有关[^Yeom2018][^Blanco-Justicia2023]，但对于大模型而言却不一定，因为大模型使用海量的数据进行训练，而且往往只训练很少的epoch次数，所以很难达到过拟合，但一些攻击仍然说明大模型可以记住训练数据。

[^Shokri2017]: [[1610.05820] Membership Inference Attacks against Machine Learning Models (arxiv.org)](https://arxiv.org/abs/1610.05820)
[^smith2023]: [[2310.01424] Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey (arxiv.org)](https://arxiv.org/abs/2310.01424)
[^Yeom2018]: [[1709.01604] Privacy Risk in Machine Learning：Analyzing the Connection to Overfitting (arxiv.org)](https://arxiv.org/abs/1709.01604)
[^Salem2018]: [[1806.01246] ML-Leaks：Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models (arxiv.org)](https://arxiv.org/abs/1806.01246)
[^Blanco-Justicia2023]:[[2206.04621] A Critical Review on the Use (and Misuse) of Differential Privacy in Machine Learning (arxiv.org)](https://arxiv.org/pdf/2206.04621)
### 版权保护
版权保护和隐私保护很像，都是不希望生成的内容里面包含特定的信息，但版权保护的要求比隐私保护更明确。隐私保护希望个人信息不要以任何方式生成，而版权保护希望保护内容不要逐字生成，即不要一模一样地生成。因为隐私保护法并不保护事实，仅保护事实被表达地方式。因此，任何准备提取“准确”信息的信息提取攻击都足以检查版权法的合规性（侵犯版权需要提取逐字信息），但它们不足以检查隐私合规性（即使提取的有关某人的信息已被改写，隐私也可能受到侵犯）。
### 模型鲁棒性
大模型训练流程可能包括使用公开数据预训练，使用公开、众包或专有数据进行微调，以及可能还要使用公开，众包或者闭源数据进行RLHF微调。在所有的训练阶段中都有可能处理低质量信息。
比如，用于预训练的数据可能包含错误，不一致或者过时的信息。用于微调的数据可能是众包的，可能会有恶意用户提供错误信息。RLHF训练依赖人类评价，粗糙的评价可能会有错误信息。
流程中提供的过时，错误，异常，噪声和恶意等信息可能影响学习过程，由此产生具有潜在漏洞的低质量模型。遗忘方法可能用于改正以上的部分问题。
### 与人类价值观对齐
训练数据的内容可能与当前社会价值观不符，包括基于性别、种族、种族、性取向或年龄的歧视性语言，通常表现为刻板印象或偏见。比如有研究发现了性别代词与职业之间的强关联。训练数据的内容还有可能包含歧视相关的仇恨，暴力或有害言论。使用包含了这些内容的数据预训练的模型可能不止会保留这些偏见有害行为，还有可能在某种情况下加剧这些行为。

机器学习模型，不仅必须保护个人隐私，还应遵循道德价值观和原则[^UN1948]，以及法律和社会规范。与不歧视、公平、仁慈和非恶意等原则保持一致至关重要。现有法律法规也开始关注机器学习技术的安全与规范[^EC2019]，因此与道德价值观对齐可能向模型遗忘方法提出要求，可以采用遗忘方法来识别和消除这种歧视性或有害行为的根源，使模型与现行的社会规范保持一致。

[^UN1948]: [Universal Declaration of Human Rights(OHCHR)](https://www.ohchr.org/en/universal-declaration-of-human-rights)
[^EC2019]: [Ethics guidelines for trustworthy AI (europa.eu)](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)
## 遗忘任务的类型 
### 普通遗忘要求
《通用数据保护条例》（GDPR）第17条规定的“被遗忘权”指出：“数据主体有权要求控制者立即删除与其相关的个人数据，在以下任一情形出现时，控制者有义务立即删除这些个人数据。......”

对于google而言，实施方式主要围绕从搜索结果中移除互联网上存在的（且非数据主体自行发布的）包含数据主体个人信息的文件。**这种删除并不意味着必须从搜索结果中删除数据主体的所有信息，而仅删除数据主体请求删除的信息。** 这如何适用于大型语言模型（LLMs）尚需进一步分析。

如果LLM在某些提示语境下能够返回特定信息，则认为这些信息是可以从LLM中提取的。将此定义与谷歌（及其他服务）实现的遗忘请求联系起来，就相当于提供一系列prompt，从而返回与数据主体相关的私人数据。
### 数据项遗忘请求
这种类型的任务要求遗忘模型中包含的一项或多项特定的数据节点或样本。这种任务在面对表格或图像数据的时候很常见，因为这些数据中的数据节点定义很明确，比如表格中的一行数据或者一张图片。但在自然语言处理中数据节点之间的区别就没那么明显，一个token可以是一个数据节点，整个句子，整个文档也可以是一个数据节点。
### 特征或概念遗忘请求
这种任务要求模型遗忘关于给定主题的所有信息，这些信息可能分布在不同的句子和文档中，比如有问题研究如何让模型遗忘关于哈利波特的所有知识[^eldan2023]。可以采用类似的方法来遵守隐私要求，从模型中删除有关数据主体的所有信息。

[^eldan2023]:[[2310.02238] Who's Harry Potter? Approximate Unlearning in LLMs (arxiv.org)](https://arxiv.org/abs/2310.02238)

### 类遗忘请求
这类任务要求移除一个类别的所有信息，在面部识别之类的识别人的场景下很常见，比如在某人离职后要求遗忘所有能够识别这个人的信息，让模型无法继续识别这个人。一个简单方法是将对应类别的logit概率归0，但也仅在黑盒场景下有效。

类移除任务在NLP场景下也有效，比如情感识别，要求移除关于某一情感的所有信息。对于生成模型，每个类对应了词表中的一个单词或token。
### 任务遗忘请求
大型语言模型（LLMs）是在大型文本数据集上进行预训练的，这些数据集包含通用任务，如下一个单词预测或掩码语言建模。在预训练之后，会对模型进行微调，以教它们执行不同的任务，如摘要、情感分析、代码编写、对话文本生成等。任务删除请求试图让微调后的LLMs忘记模型已学会执行的一个或多个任务。

那么遗忘请求类型和遗忘动机的对应关系是什么呢

| 遗忘动机  | 遗忘要求类型         |
| ----- | -------------- |
| 隐私保护  | 普通遗忘，类遗忘       |
| 版权保护  | ？              |
| 鲁棒性   | 类遗忘，任务遗忘，数据项遗忘 |
| 价值观对齐 | 概念遗忘           |

## 遗忘的要求

这里的遗忘要求是一般要求，与遗忘任务和请求无关，目的是确保遗忘过程正确执行，并且遗忘后的模型仍然具有足够的性能。
### 常用定义
一个数据集$$D$$包含$$N$$个样本$$\{x_i, y_i\}_{i=1}^N$$，其中$$x_i$$是token序列，$$y_i$$是真实标签。$$D_f\subset D$$是需要遗忘的样本集合，剩下的样本集合为$$D_r=D\setminus D_f$$。一个概率算法$$A(D)$$在给定的训练集$$D$$上输出训练好的模型，因为算法的概率本质，我们无法保证在同一个训练集上跑两次算法可以得到相同的模型，但我们可以定义一个所有可能返回的模型的概率分布$$P(A(D))$$。$$Dist(\cdot,\cdot)$$是一个用于度量两个概率分布的距离的距离函数，比如KL散度。
一个遗忘算法$$F(D_f,A(D))$$是一个（概率）算法，得到一个将$$D_f$$中的影响从概率算法$$A(D)$$的结果中移除的模型，原来定义中$$D$$也是遗忘算法的参数，但在语言模型中训练集不一定一直可以获得，所以在定义时没有包括训练集$$D$$。
### 遗忘要求
**遗忘保证**：遗忘保证作为一种理论证明，提供了与遗忘请求相关的内容被遗忘的保证，并伴随着一定程度的确定性。根据遗忘保证的强度，可以将其分为严格遗忘，近似遗忘和无保证遗忘三种
- **严格遗忘**：如果$$Dist(P(F(D_f,A(D))), P(A(D_r)))=0$$，即如果遗忘算法$$F(D_f,A(D))$$得到的结果与在剩余数据上训练得到的模型一致，则说遗忘算法可以完成严格遗忘。
- **近似遗忘**：如果$$Dist(P(F(D_f,A(D))), P(A(D_r)))\leq t$$，即如果遗忘算法$$F(D_f,A(D))$$得到的结果与在剩余数据上训练得到的模型之间的差别可以确定在一定范围中，则称遗忘算法可以完成近似遗忘。在给定了参数$$\epsilon$$和一个样本$$z\in D$$之后，可以定义$$\epsilon-$$遗忘为$$e^{-\epsilon}\leq\frac{Pr(F(z, A(D)))}{Pr(A(D\setminus z))}\leq e^{\epsilon}$$，即两个模型的概率分布的差异在$$[e^{-\epsilon}, e^{\epsilon}]$$之间。
- **无保证遗忘**：以上的遗忘保证可能无法获得，这种情况下一般使用经验验证或者审计来提供一个风险削减级别。具体方法与具体的遗忘要求相关。

**泛化能力**：大模型遗忘场景中的$$D_f$$不一定来自于原始的训练语料。需要遗忘的样本可以是一个普遍的概念而不是一个特定的训练样本，因此需要遗忘方法可以泛化到有相同特征的类似样本上，由此强化概念遗忘的效果，并且提升面对*释义攻击*的鲁棒性。

**性能保留**：遗忘后的模型应当尽可能保留原模型的能力，不能遗忘完把模型忘废了。在遗忘某些任务之后，某些benchmark可能就不适用了。**一致性**定义为遗忘后的模型和用剩余数据重训的模型在预测结果上的相同程度，**精确性**定义为遗忘后的模型的预测准确程度[^Xu2023]。尽管这些指标不能直接用于生成模型，但类似困惑度之类的指标可以用于比较遗忘模型和重训模型

**运行时间和可延展性**：遗忘算法应当可以被及时执行，从而使个人隐私信息无法在额外时间被访问。其他遗忘需求，类似于删除版权保护的材料，纠正偏见或者删除任务可能相对没有这么紧急。可延展性讨论有多少个遗忘需求可以被同时执行，以及遗忘会如何影响运行时间和可用性。

[^Xu2023]: Machine unlearning: A survey 

## LLM中的数字遗忘方式
### 数据预处理和模型重训
第一种方法是仔细检查并选择预训练和微调阶段使用的数据，从中去掉和隐私相关的数据。Meta训练Llama2的时候限制了使用具有大量隐私信息的数据，他们分析了数据中的性别，国籍，性取向，宗教等潜在偏见，但并没有对数据做删减。还使用了HateBert对仇恨内容进行了分析，发现有$$0.2\%$$的内容是潜在有害的。

第二种方法是通过文本净化（或者文本匿名化）来限制训练阶段的隐私信息数量。一般来说是手动完成的，但近期也有自动化的方法，通过命名实体识别来识别潜在的隐私或敏感项，然后从数据中删除或匿名化[^Sanchez-Batet2016,^Hassan2019]

第三种方法是去重，即找到并删除训练数据中的重复文本，重复文本更容易受到成员推理攻击，因此去重可以卡诺一个避免模型记忆（可能导致隐私，版权，鲁棒性或者对齐问题）的有效方法。

[^Sanchez-Batet2016]: C-sanitized: A privacy model for document redaction and sanitization.
[^Hassan2019]: Automatic anonymization of textual documents: detecting sensitive information via word embeddings.

### 隐私保护的预训练方法
隐私保护的预训练方法可以限制单一节点对于模型的影响，目的是用隐私训练方法保护隐私性，而不是像前面方法对于数据进行挑选
- 差分隐私随机梯度下降法（DP-SGD）[^dp-sgd]：差分隐私方法限制了能够从数据集中推理出单个对象的正确信息的概率，如果对象是一条记录，对象出现和消失之后差分隐私机制的输出应当几乎不变。机器学习中差分隐私一般通过DP-SGD训练方法实现，一般实现的是一个宽松方法$$(\epsilon,\delta)-DP$$，可以以$$(1-\delta)$$的几率满足$$\epsilon-DP$$。在大模型中或者广义的文本数据保护中，非常难以定义哪些是需要保护的个体对象，由此限制了DP-SGD的应用
- 教师集合隐私聚合（PATE）：PATE[^pate]在不同的私有数据子集上训练多个模型（即教师模型），然后将这些模型的预测结果以一种保护隐私的方式聚合起来，以标记新训练实例，从而训练一个学生模型。这种方法的核心优势在于，它能够在不直接访问私有数据的情况下，利用这些数据训练出一个性能良好的模型。PATE框架通过在聚合教师模型的预测时引入噪声，来保护数据的隐私性。

[^dp-sgd]: Deep learning with differential privacy
[^pate]: Scalable private learning with pate 

### 机器遗忘
因为大模型高昂的训练成本，重新训练来消除不需要的行为开始不切实际了。机器遗忘用于从LLM中移除已经经过预训练或者微调的知识，而不进行重训。这些机制依赖于进一步的微调，通常伴随着对抗性目标，用于识别与不需要的信息相关的参数及其修改，以及参数的算术运算。
### Prompt工程
指定的prompt可以在微调后进一步控制模型行为，这些方法并不修改模型参数，并且可以通过输入对抗prompt绕过。但在生成模型对话的开头插入精心构建的prompt可以使其避免输出隐私，偏见或有害内容。比如[^ustun2024]中使用prompt来要求模型判断内容是否有害，并且在有害时拒绝回答，拒绝了88%的有害请求。

[^ustun2024]: Aya model: An instruction finetuned open-access multilingual language model.

### 后处理
有些模型可能只能通过API访问，在模型生成回答并返回给用户之前，服务提供商可以先分析回答来找到不合适的生成内容，比如使用基于大模型的分类器。其他方法包括使用一个不合适的回复的存储来识别和过滤类似的生成结果。
### 生成式LLM中的采样策略
虽然不是遗忘策略，但采样策略中的选择可以影响逐字生成训练数据中的序列的概率。采样策略包括贪心策略，top-k采样（从概率最高的k个词中采样），多模态采样（下一个token从整个概率分布中采样）。温度参数被用于拉平概率分布，logits在进行softmax之前首先减去温度参数，使得模型更容易生成多样化的内容。
## LLM中的机器遗忘方法
LLM中的遗忘方法可以分为全局权重调整，本地权重调整，结构调整和输入/输出调整方法。全局权重调整有能力修改模型中的所有权重，而本地权重调整仅能修改一个特定的权重子集。结构调整向模型结构引入额外的层，输入输出调整方法在输入输出层执行。
### 全局权重调整
有更强的遗忘保证，但也有更强的计算和时间消耗
#### 数据分片
数据分片方法主要包括将训练数据分城若干不相交的片（shard），然后为每个分片训练一个单独的模型，这些模型可以后续用于有效地移除遗忘数据。SISA[^SISA]是一个精确遗忘的通用框架，训练数据被分为$$S$$个不相交的分片，每个包含$$R$$个slice，在每个分片上通过逐渐添加slice的方式训练$$R$$个模型。在推理时每个模型预测一个标签，然后进行标签聚合。在遗忘时首先确定遗忘要求设计的分片和slice，然后在最后一个checkpoint的基础上使用移除了遗忘数据点后的数据重新训练。SISA对于LLMs来说并不十分实用，因为与模型和检查点保存、再训练和推理相关的高计算/内存成本。增加推断成本的同时，由于损失了训练样本间的协同信息，降低了集成模型的效用。kadhe等人[^Kadhe2023]发现SISA确实可以降低LLMs中的公平性，并采用了一种后处理公平性改进技术，使其更加公平。
LOO[^LOO]使用leave-one-out策略来降低SISA的训练成本并保持推理可用性，依然训练$$M$$个不同的老师模型，但在遗忘时，学生模型使用遗忘要求不涉及的$$M-1$$个老师模型的预测来进行微调，微调的学生模型用于后续的预测。虽然相较SISA可以提升可用性，但只能为学生模型提供近似遗忘保证，老师模型则没有遗忘保证。其次预测和聚合软标签，然后微调整个基础模型的参数的过程会产生显著的计算开销，特别是在LLMs中。

[^SISA]:Machine unlearning.
[^Kadhe2023]:Fairsisa: Ensemble postprocessing to improve fairness of unlearning in llms.
[^LOO]:Forgetting private textual sequences in language models via leave-one-out ensemble 

#### 梯度上升
主要想法是微调所有模型参数来最大化在目标token上的损失。在有代表敏感信息的目标token序列后，Jang等人[^jang2022]对这些token序列的原始损失函数进行削弱。给定一个模型$$f(x;\theta)$$和一个token序列$$x=(x_1,...,x_T)$$，对$$x$$的损失可以表示为

$$
\mathcal{L}_x(\theta)=-\sum_{t=1}^T\log(p_{\theta}(x_t\vert x_{<t}))
$$

$$N$$个样本的整体损失为

$$
\mathcal{L}(\theta)=\frac{1}{N}\sum_{i=1}^N\mathcal{L}_{x^i}(\theta)
$$

因此模型参数根据梯度上升规则更新为

$$
\theta=\theta+\eta\nabla_{\theta}\mathcal{L}(\theta)
$$

作者发现一次性地遗忘多个样本会显著降低LM的性能，而顺序地遗忘这些样本可以减轻LM的性能下降。方法在文本分类和对话任务上进行测试，并用提取相似度[^jang2022]和记忆准确性[^triumala2022]进行评估。梯度上升方法仅需要需要被遗忘的数据，并且某些时候可以强化模型的泛化能力[^Yoon2023]，但也可以导致模型失去对语言的理解[^eldan2023]，而且遗忘的成功取决于特定的目标数据以及需要被遗忘的数据的领域[^smith2023]。
SeUL方法[^seul]在特定的敏感分块（span）上而不是整个token序列上使用梯度上升来保留LLM的泛化能力，提出了两种选择敏感span的标记方法，在线方法认为拥有低预测概率（即高困惑度）的token是敏感的，离线方法使用了上下文学习能力来标记敏感span。作者使用了敏感提取相似度和敏感记忆准确度来度量在文本分类和对话任务上的遗忘效果，尽管SeUL相较GA支持保留可用性，有效的遗忘，但标记方法可能在识别隐私token的准确性上有所限制。
Yao等人[^yao2023]发现仅使用梯度上升无法有效遗忘不合适的行为，在正常样本上保持性能比遗忘更难实现，用于指导LLMs在正常任务上保持效用的正常数据格式极大地影响了正常性能。并在此观察的基础上提出了一个最小化三个损失函数的方法，通过同时( 1 )在遗忘样本上应用梯度上升，( 2 )在遗忘样本上强制随机输出，( 3 )最小化原始模型和遗忘模型在正常样本上的预测之间的KL散度，以保持正常效用来更新LLM完成遗忘。作者发现强制随机输出可以帮助模型遗忘不想要的输出。虽然可以在遗忘和模型可用性之间获得更好的平衡，方法需要大量的训练批次来同时完成遗忘和可用性保留。

[^jang2022]: Knowledge unlearning for mitigating privacy risks in language models.
[^triumala2022]: Memorization without overfitting: Analyzing the training dynamics of large language models.
[^Yoon2023]: Gradient ascent post-training enhances language model generalization.
[^seul]: Selective forgetting: Advancing machine unlearning techniques and evaluation in language models.
[^yao2023]: Large language model unlearning.

#### 知识蒸馏
知识蒸馏方法将遗忘模型作为学生模型，模仿拥有想要的行为的老师模型。知识差距对齐（KGA）[^kga]方法利用训练数据、待遗忘数据和外部数据进行遗忘，以产生一个更新的模型，该模型在遗忘数据上表现出与在未见数据上相似的行为，同时保留对其余数据的可用性。知识差距（knowledge gap）指使用不同的数据训练的模型在预测分布上的差异。对齐是通过最小化遗忘模型在遗忘数据上的预测和原始模型在未知数据上的预测的分布差异（KL散度）完成的。为保持模型可用性，在处理保留数据时，将原模型作为遗忘模型的老师，以最小化分布差异。知识蒸馏方法的优点在于可以适用于不同的模型和任务，但需要训练两个模型然后微调所有模型参数的方式限制了在大模型上的适用性。遗忘需要训练数据，遗忘数据和与遗忘数据没有重合的外部数据，遗忘模型的可用性与遗忘数据和外部数据的规模高度相关。

[^kga]: Kga: A general machine unlearning framework based on knowledge gap alignment.

#### 通用替代
这种方法主要通过微调整个模型的参数来预测通用的替代项，而不是预测那些需要被遗忘的标记，以此实现遗忘。Eldan等人[^eldan2023]提出了一个新的遗忘方法，成功遗忘了所有有关哈利波特书籍的知识。整个方法分为三步
- 通过强化模型识别token：使用一个对遗忘内容有更深刻理解的强化模型来识别要遗忘的token，在微调原来模型的基础上，有明显概率提升的token识别为与内容相关需要遗忘的token
- 表达替换：目标数据中的独特表达替换为通用的替代标签，替代标签使用gpt-4自动完成。有助于近似一个没有见过目标数据的模型的预测概率
- 微调：模型使用替代结果微调来移除原始文本的记忆并在收到相关上下文的prompt时提供一个可信的替代。
方法成功遗忘了哈利波特书籍相关的信息，但Shi等人[^Shi2023]发现使用这个方法遗忘的模型依然有可能输出相关版权的内容。这个方法依赖于使用通用项替代特殊项，哈利波特书籍中特殊项很多，但在非小说的文本中往往要遗忘的是概念和意识而不是特殊的单词。

[^Shi2023]: Detecting pretraining data from large language models.

#### RLHF
Quark[^quark]通过在不要做的信号上微调模型来使模型遗忘不想要的行为。Quark在量化阶段使用奖励信号对数据池进行排序并划分为分位数。在学习阶段在分位数划分的数据池上使用标准的目标函数和KL惩罚项训练模型，在探索阶段，方法通过从模型在高奖励token上的采样向数据池中添加新的生成内容。三个阶段的目标是教导模型生成关于奖励token的不同质量的文本，然后在推理阶段，采样根据最佳奖励的token来控制进行期望的生成。虽然是最先进的与人类期望对齐的生成控制方法，Quark在数据池规模增大时计算开销会显著提升，并且模型可能仍旧在参数中保留敏感信息。

[^quark]: Quark: Controllable text generation with reinforced unlearning.

### 本地权重调整

#### 本地重训
本地重训方法使用选择性重训策略，只有和遗忘目标相关的参数才会被重训，其他参数不变。划分对比梯度遗忘方法（PCGU）[^PCGU]系统性的定位并重训与偏见行为相关的模型参数。方法包括评分排序权重并根据对句子对的梯度选择权重。对比句子对包含两个相似但其中一个引入了偏见的句子。在这些句子上的梯度用于定位哪些权重明显参与了偏见。方法在缓解性别职业偏见和将影响泛化至未见的领域上均有效，但仅考虑了在masked LLM上的偏见问题，所提出的方法是否可以推广到其他类型的LLMs和更复杂的社会偏见，如种族主义和阶级主义，仍然是不确定的。

[^PCGU]: Unlearning bias in language models by partitioning gradients.

#### 任务向量
任务向量方法基于大模型可以被分解为任务相关的向量并且消除遗忘任务的向量就可以完成遗忘的思想。Iiharco等人[^taskvector]提出了一个基于任务向量控制神经网络行为的范式。任务向量$$\tau_t$$定义为在特定任务$$t$$上微调后的模型$$\theta_{ft}^t$$和对应的预训练模型$$\theta_{pre}$$之间的参数差异。使用任务向量可以对和任务$$t$$相关的模型行为进行选择性调整，而不会明显影响其他任务：减去任务向量可以使模型忘记任务相关的知识，加上任务向量可以提升在任务上的表现。但是它无法处理遗忘离散的事实或特定的令牌序列，这使得它比细粒度的修改更适用于更广泛的基于任务的修改。
F-learning[^Flearning]在任务向量的基础上提出，原始模型首先在旧知识上微调，然后再从初始模型参数中减去微调模型和初始模型的参数差异，这个过程被称为“旧知识遗忘”。然后遗忘后的模型再使用新知识微调，称为“新知识学习”过程。这个过程有效缓解了新知识和旧知识之间的差异，但与任务向量方法类似，F-learning方法也无法忘记单独的事实或者token序列。

[^taskvector]:Editing models with task arithmetic.
[^Flearning]:Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models.

#### 直接修改
这种方法直接修改相关参数或者神经元来完成遗忘。DEPN[^depn]假设隐私信息是编码在大模型的隐私相关神经元内的。基于这个假设，方法首先使用整体梯度法[^Sundararajan2017]找到这些神经元，然后将它们的激活之设为0来消除其中编码的隐私信息。方法还引入了一个隐私神经元聚合器来处理多个遗忘请求。模型大小、训练时间以及私有数据的出现频率都是影响模型记忆的因素。随着模型对私有数据记忆的加深，与这些数据相关的隐私相关神经元的聚合变得更加明显。方法仅需要忘记集而无需微调。然而，随着需要忘记的数据量增加，模型的效用显著下降，因为更多的神经元被停用。此外，一批中的实例太多会减少遗忘的效果。
选择性裁剪方法[^selectprune]发现大模型中前馈和注意力神经元都是任务相关的，从中移除部分神经元可以有效降低在遗忘任务上的表现，而几乎不影响其他任务的表现，因此选择性裁剪方法从大模型中识别并移除与特定能力相关的神经元。这个方法在识别和移除特定任务的神经元时是计算和数据有效的，也有一处其他有害技术的潜在能力。但这个方法需要任务表示的数据集，并且只在这些数据集直接捕获的能力上有效。方法的有效性取决于大模型能力的分离程度，在像Pythia没有dropout的模型和小型大模型上效果一般。

[^depn]:Depn: Detecting and editing privacy neurons in pretrained language models.
[^Sundararajan2017]:Axiomatic attribution for deep networks.
[^selectprune]: Dissecting large language models.

### 结构调整

这些方法想模型中添加额外参数，旨在保留遗忘效率和模型可用性。

#### 额外的可学习层

EUL[^eul]在transformer的前馈层后添加了一个额外的遗忘层，对每个遗忘请求，遗忘层使用选择性的老师-学生目标函数单独训练。在训练遗忘层时，原始的模型参数被冻结，并使用一个联合目标，目的是完成遗忘目标（使遗忘数据的预测结果与在原模型上的预测结果尽可能不同）同时在其他数据上与原模型尽可能一致。此方法并不构成完全遗忘，因为移除附加层会导致旧模型在被遗忘的数据上恢复到原来的行为，从而与隐私法则相矛盾（因为训练的时候原模型参数被冻结，没有做修改）。同时，需要训练一个额外的层来忘记每个遗忘请求，这可能是不可扩展的。
Kumar等人[^Kumar2022]提出了两个SISA的变体来提升在大模型上的效率：SISA-FC和SISA-A。SISA-FC从预训练模型开始，首先在最后添加一个全联接层，后面只有新添加的层需要重训，通过这个方法来提升训练效率，降低存储要求。但模型可用性相比全参数微调会被严重削弱。SISA-A通过使用参数高效适配方法[^houlsby2019]来改进这个问题，即将可学习模块嵌入transformer的编码器模块。SISA-A的模型可用性更好，但也比SISA-FC的训练消耗更高。两个方法的共同缺点是预训练阶段学习的通用文本语料库无法被遗忘。

[^eul]:Unlearn what you want to forget: Efficient unlearning for llms
[^Kumar2022]:Privacy adhering machine un-learning in nlp.
[^houlsby2019]:Parameter-efficient transfer learning for nlp.

#### 线性转换层

LEACE提出了一种闭式解方法，通过对表征的最小化修改来避免线性分类层检测到某些概念，通过应用一个称为“概念擦除”的过程，擦除每一个中间表示中关于目标概念的所有线性信息。LEACE可以用于提升公平性和可解释性，但也有可能减弱可用性，因为会擦出不相关特征，并且还需要在训练中缓存隐藏状态，需要额外的存储要求。并且有效性限制在part-of-speech，可用性没有被验证。
有偏见的LLM的表征中的性别信息可以被分为事实性别信息和性别偏见。事实性别信息包括语法或语义特征，但性别偏见指模型将某些单词与特定性别关联的倾向。Limisiewicz等人[^Limisiewicz2022]尝试通过修改上下文嵌入来缓解预训练模型的性别偏见。他们使用一个正交变换来分离模型嵌入中编码的词法和句法信息，然后从嵌入空间中过滤出偏见子空间并且保留事实性别信息的子空间。尽管这种方法仅在上下文嵌入上进行线性变换因此很高效，但无法保证所有偏见相关的维度被过滤掉了。因为偏见信号可能被非线性编码在模型中，及时整个子空间被移除了，信息可以从模型的下一层中恢复出来。

[^Limisiewicz2022]:Don’t forget about pronouns：Removing gender bias in language models without losing factual gender information.

### 输入输出调整
此类方法将大模型看作黑盒，仅要求能够接触模型的输入输出而不要求深入模型内部工作流程。这种方法在某些用户无法接触模型权重的场景非常实用（比如基于API的模型），但从隐私保护法条（RTBF）的角度而言，这种方法不提供任何真实的隐私保护能力，因为模型仍旧能够保留理应遗忘的知识。

#### 输入操作
zero-shot self-debiasing方法[^zssdb]利用大模型的zero-shot能力，提出了一种基于prompting的偏见缓解方法来减轻有偏见的刻板印象的预测，包含两种方法：通过解释的自去偏见和通过重新prompt的自去偏见。通过解释的自去偏见中，模型首先解释答案选择中的无效假设，识别潜在的刻板印象，然后回答问题。通过重新prompt的自去偏见中，模型首先根据极限方法回答问题，然后被重新prompt来从初始回答中移除偏见。这种方法不需要额外的训练数据，示例回复，微调，或者额外的模型，因此更加有效可行。但这种方法是任务和上下文相关的，针对多项选择任务设计而不是向通常的开放性问答任务。并且它使用了人工设计的prompt，限制了向其他偏见泛化的能力。

[^zssdb]:Self-debiasing large language models: Zero-shot recognition and reduction of stereotypes.

#### 信息检索
信息检索方法旨在选择性地提取或者控制外部知识的信息来影响大模型遗忘的路径。
SERAC方法[^serac]使用基于记忆的模型编辑方法，将大模型当作一个黑盒模型，方法作为大模型的一个装饰器，包含三个主要部件：一个显式的编辑缓存，一个辅助的范围分类器，和一个反事实模型。当有知识编辑请求时，封装模型分两步预测一个新的输入。首先范围分类器会评估新输入在每个缓存的编辑的范围中的可能性，如果在某个范围中，最高可能性的编辑结果会被检索出来，然后反事实模型会根据新输入和检索到的编辑来预测。如果新输入没有在任何一个范围内，则输出基础模型的预测。这种方法简单且易于实施，并且不需要对原始模型进行修改。此外，它可以编辑具有不同架构的多个模型。然而，它可能容易受到检索错误的影响，例如噪声和有害内容，以及知识冲突问题[^Zhang2024]。此外，它依赖于编辑数据集进行训练，在某些设置中可能需要更多的计算和内存资源。
为了在不重训的前提下通过用户交互修改模型错误，MemPrompt[^memprompt]为模型搭配了一个记录模型误解了用户意图的记忆模块，系统将反馈的记忆保存为一组键值对，键是误解的输入，值是矫正结果。给定新的prompt时，系统首先在记忆中搜索类似的输入，检查是否犯过类似的错误，如果有的话就会将矫正后的结果加入prompt。这种方法虽然有效，但可能在用户交互增长时面对记忆模块的延展性以及保持可用性的问题。

[^serac]: Memorybased model editing at scale.
[^Zhang2024]: A comprehensive study of knowledge editing for large language models.
[^memprompt]: Memory-assisted prompt editing to improve gpt-3 after deployment.

#### 上下文学习
ICUL[^icul]在推理阶段构建了一个特定的上下文，让模型分辨是否在训练时从来没见过特定的数据节点。ICUL包含三步
- 标签翻转：给定遗忘请求，对应训练节点上的标签会被翻转，得到一个新的模板
- 添加标签正确的训练节点：在需要遗忘的节点外，随机采样多个标签正确的节点对并添加入模板
- 预测：查询请求添加乳模板，构建最终的prompt，然后模型使用$$0$$温度预测下一个token

标签反转旨在移除特定的训练样本对模型输出的影响，第二步旨在削弱标签翻转的影响。ICUL使用了LiRA-Forget测试进行评估，并且偶尔超越了白盒方法。但其有效性依赖于模型的上下文学习能力，并且仅在标签翻转可行的文本分类任务上进行了测试。

[^icul]: In-context unlearning: Language models as few shot unlearners