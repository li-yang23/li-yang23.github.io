---
title: 大模型遗忘论文阅读杂记
description: notes for llm unlearning papers do not need to write a whole blog
tags: ai-explanation llm-unlearning
category: AI
date: 2024-12-02
---
## 全局参数调整

### 2023

#### [EMNLP 23] Knowledge unlearning for mitigating privacy risks in language models
预训练语言模型（LMs）在初始预训练过程中会记忆大量的知识，包括可能侵犯个人生活和身份隐私的信息。以往针对LMs隐私问题的工作主要集中在*数据预处理*和*差分隐私*方法上，这两者都需要重新训练底层的LM。本文提出知识遗忘作为一种替代方法，以减少LMs事后的隐私风险。本文通过简单地对目标token序列执行梯度上升，在不降低大型LMs的一般语言建模性能的情况下忘记目标序列。本文发现顺序遗忘比一次性尝试遗忘所有数据更有效，并且遗忘高度依赖于被遗忘的数据类型（领域）。通过与已知可以减轻LMs隐私风险的先前方法进行比较，本文方法可以在已知数据容易受到提取攻击的场景中提供更强的经验隐私保证，同时更加高效和稳健。

**本文的背景**

攻击者可以通过数据提取攻击从预训练模型中获取包括个人身份信息在内的各种隐私信息（AI聊天机器人Iruda在2021年成为了第一个因为违反个人信息保护条例而被起诉的AI系统）。随着模型规模扩大，提取训练数据变得更加容易。目前行业做法是发布包含几十亿个参数的超大模型供公众使用，因此让大模型提供隐私保护保证就显得愈发重要。

**本文要解决的问题**

法律法规的被遗忘权要求企业在收到个人要求后从模型中删除对应的个人信息。

**原来的方法为什么不行**

原来的一般做法是在训练之前就从数据中删除个人隐私信息，或者设计满足差分隐私的算法。但这两种方法在收到个人遗忘要求后都需要重新训练模型，导致这两种方法无法有效应用于训练成本超高的大模型上。此外，数据预处理方法假设隐私信息易于识别、指定和删除，而DP算法只能保证对具有明确隐私边界的信息的保护，这使得它们在现实场景中存在不足，因为每个人的隐私标准可能不同。

**本文的方法是什么**

使用*知识遗忘*作为替代方法，对少量的参数进行微调，而不是重新预训练整个模型。具体的遗忘方法是反向优化，即将梯度下降方向逆向，最大化损失函数。
$$
\mathcal{L}_{UL}(f_{\theta}, \mathbb{x})=-\sum_{t=1}^T\log(p_{\theta}(x_t\vert x_{<t}))
$$

提出了两种度量指标来衡量语言模型的隐私泄露风险：提取相似性和记忆准确度