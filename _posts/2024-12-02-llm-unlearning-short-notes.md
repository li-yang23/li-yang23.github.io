---
title: 大模型遗忘论文阅读杂记
description: notes for llm unlearning papers do not need to write a whole blog
tags: ai-explanation llm-unlearning
category: AI
date: 2024-12-02
---
## 全局参数调整

### 2023

#### [NIPS 23][Retrain] FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs
训练大型语言模型（LLMs）在时间和计算资源方面是一项成本高昂的工作。在无监督预训练阶段使用大量的训练数据使得验证所有数据变得困难，不幸的是，训练过程中可能会摄入不良数据。从头开始重新训练是不切实际的。然而，任何修改都可能改变LLMs的行为，尤其是在公平性等关键维度上。本文评估了SISA的性能-公平性的权衡，并实证展示了SISA确实可以减少LLMs中的公平性。为了解决这个问题，本文为SISA产生的集成模型提出了后处理偏见减轻技术。我们引入了一个后处理公平性改进技术，设计了三种可以处理模型集成的方法，并证明了其中一种方法是模型集成的最优公平预测器。通过实验结果，我们展示了我们称为'FairSISA'的后处理框架的有效性。

**如何评估的公平性**

使用毒性文本分类任务，以群组公平性度量模型公平性。首先指定敏感话题，如果一个文本样本包含一个或多个与敏感话题相关的术语，就认为样本属于敏感群组，否则属于互补群组。然后分析在存在/不存在敏感信息的情况下，有毒文本预测的公平性。如果公平的话，那么模型的预测表现应当不受敏感话题的影响。

**如何缓解的**

引入了HPS方法，构造一个仅取决于预测标签和敏感属性的派生预测，并在最小化分类损失的同时满足等化几率（equalized odds）。

#### [Arxiv 23][Retrain] Forgetting private textual sequences in language models via leave-one-out ensemble
近期研究表明，语言模型倾向于记忆训练语料库中罕见或独特的标记序列。部署模型后，实践者可能会被要求根据个人请求从模型中删除任何个人信息。每次个人想要行使被遗忘权时，都重新训练底层模型在计算上是昂贵的。我们采用了教师-学生框架，并提出了一种新颖的留一法(leave-one-out, LOO)集成方法，以使模型忘记需要被遗忘的目标文本序列。在我们的方法中，多个教师在不相交的集合上进行训练；对于每个需要移除的目标序列，我们排除在包含该序列的集合上训练的教师，并聚合剩余教师的预测，以在微调期间提供监督。在LibriSpeech和WikiText-103数据集上的实验表明，所提出的方法比其他对应方法实现了更优越的隐私-效用权衡。

一共训练老师+1个模型，老师使用分片训练子模型并不部署，基础模型使用所有数据并部署。

#### [EMNLP 23][Gradient Ascent] Knowledge unlearning for mitigating privacy risks in language models
预训练语言模型（LMs）在初始预训练过程中会记忆大量的知识，包括可能侵犯个人生活和身份隐私的信息。以往针对LMs隐私问题的工作主要集中在*数据预处理*和*差分隐私*方法上，这两者都需要重新训练底层的LM。本文提出知识遗忘作为一种替代方法，以减少LMs事后的隐私风险。本文通过简单地对目标token序列执行梯度上升，在不降低大型LMs的一般语言建模性能的情况下忘记目标序列。本文发现顺序遗忘比一次性尝试遗忘所有数据更有效，并且遗忘高度依赖于被遗忘的数据类型（领域）。通过与已知可以减轻LMs隐私风险的先前方法进行比较，本文方法可以在已知数据容易受到提取攻击的场景中提供更强的经验隐私保证，同时更加高效和稳健。
