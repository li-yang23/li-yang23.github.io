---
title: 大模型遗忘论文阅读杂记
description: notes for llm unlearning papers do not need to write a whole blog
tags: ai-explanation llm-unlearning
category: AI
date: 2024-12-02
---
## 全局参数调整

### 2023

#### [NIPS 23][Retrain] FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs
训练大型语言模型（LLMs）在时间和计算资源方面是一项成本高昂的工作。在无监督预训练阶段使用大量的训练数据使得验证所有数据变得困难，不幸的是，训练过程中可能会摄入不良数据。从头开始重新训练是不切实际的。然而，任何修改都可能改变LLMs的行为，尤其是在公平性等关键维度上。本文评估了SISA的性能-公平性的权衡，并实证展示了SISA确实可以减少LLMs中的公平性。为了解决这个问题，本文为SISA产生的集成模型提出了后处理偏见减轻技术。我们引入了一个后处理公平性改进技术，设计了三种可以处理模型集成的方法，并证明了其中一种方法是模型集成的最优公平预测器。通过实验结果，我们展示了我们称为'FairSISA'的后处理框架的有效性。

**如何评估的公平性**

使用毒性文本分类任务，以群组公平性度量模型公平性。首先指定敏感话题，如果一个文本样本包含一个或多个与敏感话题相关的术语，就认为样本属于敏感群组，否则属于互补群组。然后分析在存在/不存在敏感信息的情况下，有毒文本预测的公平性。如果公平的话，那么模型的预测表现应当不受敏感话题的影响。

**如何缓解的**

引入了HPS方法，构造一个仅取决于预测标签和敏感属性的派生预测，并在最小化分类损失的同时满足等化几率（equalized odds）。

#### [Arxiv 23][Retrain] Forgetting private textual sequences in language models via leave-one-out ensemble
近期研究表明，语言模型倾向于记忆训练语料库中罕见或独特的标记序列。部署模型后，实践者可能会被要求根据个人请求从模型中删除任何个人信息。每次个人想要行使被遗忘权时，都重新训练底层模型在计算上是昂贵的。我们采用了教师-学生框架，并提出了一种新颖的留一法(leave-one-out, LOO)集成方法，以使模型忘记需要被遗忘的目标文本序列。在我们的方法中，多个教师在不相交的集合上进行训练；对于每个需要移除的目标序列，我们排除在包含该序列的集合上训练的教师，并聚合剩余教师的预测，以在微调期间提供监督。在LibriSpeech和WikiText-103数据集上的实验表明，所提出的方法比其他对应方法实现了更优越的隐私-效用权衡。

一共训练老师+1个模型，老师使用分片训练子模型并不部署，基础模型使用所有数据并部署。

#### [EMNLP 23][Gradient Ascent] Knowledge unlearning for mitigating privacy risks in language models
预训练语言模型（LMs）在初始预训练过程中会记忆大量的知识，包括可能侵犯个人生活和身份隐私的信息。以往针对LMs隐私问题的工作主要集中在*数据预处理*和*差分隐私*方法上，这两者都需要重新训练底层的LM。本文提出知识遗忘作为一种替代方法，以减少LMs事后的隐私风险。本文通过简单地对目标token序列执行梯度上升，在不降低大型LMs的一般语言建模性能的情况下忘记目标序列。也就是
$$
\begin{aligned}
\mathcal{L}_x(\theta)&=-\sum_{t=1}^T\log(p_{\theta}(x_t\vert x_{<t}))\\
\theta&=\theta+\eta\nabla_{\theta}\mathcal{L}(\theta)
\end{aligned}
$$

本文发现顺序遗忘比一次性尝试遗忘所有数据更有效，并且遗忘高度依赖于被遗忘的数据类型（领域）。通过与已知可以减轻LMs隐私风险的先前方法进行比较，本文方法可以在已知数据容易受到提取攻击的场景中提供更强的经验隐私保证，同时更加高效和稳健。

> 但是在遗忘哈利波特的那片工作中发现，单纯的梯度上升可能导致模型失去对语言的理解。此外，梯度上升的效果取决于选择去上升的这部分数据的选择以及目标数据的领域

#### [ACL 23][Knowledge Distillation] Kga: A general machine unlearning framework based on knowledge gap alignment.
最近关于“被遗忘权”的立法引发了人们对机器反学习的兴趣，即学习到的模型被赋予忘记特定训练实例信息的功能，就好像它们从未存在于训练集中。以前的工作主要集中在计算机视觉场景中，在很大程度上忽略了 NLP 领域遗忘的本质，因为文本数据比图像包含更多明确和敏感的个人信息。本文提出了一个称为 KGA 的通用遗忘框架来诱导遗忘。与试图恢复梯度或强制模型接近某一特定分布的工作不同，KGA 保持了分布差异（即知识差距）。此外，本文首先将遗忘方法应用于各种 NLP 任务（即分类、翻译、响应生成），并提出了几个有针对性的反学习评估指标。在大规模数据集上的实验表明，KGA 比基线有了全面的改进，其中广泛的分析进一步验证了 KGA 的有效性并为 NLP 任务的反学习提供了见解。

#### [NeurIPS 23][Gradient Ascent] Large Language Model Unlearning
本文研究如何在大型语言模型 (LLM) 上进行遗忘，即忘记不良的不当行为。本文展示了至少三种将 LLM 与人类偏好对齐的场景可以从反学习中受益：(1) 删除有害反应，(2) 删除受版权保护的内容，以及 (3) 减少幻觉。遗忘作为一种对齐技术，有三个优点。(1) 它只需要负面（例如有害）示例，这些示例比 RLHF（从人类反馈中进行强化学习）所需的正面（例如有帮助且通常是人类编写的）示例更容易和更低成本地收集（例如通过红队或用户报告）。(2) 它在计算上是高效的；成本与轻度监督微调相当。(3) 当我们知道哪些训练样本导致不当行为时，它特别有效。

本文认为如果从业者资源有限，优先考虑的是停止生成不良输出而不是尝试生成理想输出，那么遗忘就特别有吸引力。尽管只有负样本，但消融研究表明，遗忘仍然可以实现比RLHF更好的对齐性能，而计算时间仅为RLHF的2%。

#### [Arxiv 23][General Alternatives] Who's Harry Potter? Approximate Unlearning in LLMs

大型语言模型 (LLM) 在海量互联网语料库上进行训练，这些语料库通常包含受版权保护的内容。这对这些模型的开发者和用户以及原作者和出版商构成了法律和道德挑战。本文提出了一种遗忘技术，用于从 LLM 中遗忘训练数据的子集，而无需从头开始重新训练。
本文在从 Llama2-7b 模型遗忘哈利波特书籍的任务上评估了我们的技术。在大约 1 GPU 小时的微调中，本文有效地消除了模型生成或调用哈利波特相关内容的能力，而其在常见基准测试上的表现几乎不受影响。
由三个主要部分组成：首先使用一个在目标数据上进一步训练的强化模型，通过将其logit与基础模型的logit进行比较，识别与目标最相关的token。然后用通用的对应词替换目标数据中的特殊表达，并利用模型自身的预测为每个token生成替代token。这些token旨在近似未在目标数据上训练的模型的下一个token预测。在这些替代token上对模型进行微调，这样每当模型被提示上下文时，就会有效地从模型的内存中删除原始文本。

### 2024

#### [Arxiv 24][Gradient Ascent] Selective forgetting: Advancing machine unlearning techniques and evaluation in language models

本文介绍了一种用于在语言模型中实现精确和选择性遗忘的新方法。与以前采用完全相反的训练目标的方法不同，这种方法旨在减轻对语言模型性能的不利影响，特别是在生成任务中。此外，还提出了两个创新的评估指标：敏感信息提取可能性 (S-EL) 和敏感信息记忆准确度 (S-MA)，旨在衡量敏感信息消除的有效性。为了加强遗忘框架，提出了一种注释敏感范围的有效方法，涉及在线和离线策略。在线选择机制利用语言概率分数来确保计算效率，而离线注释则需要基于大型语言模型 (LLM) 的强大两阶段过程。


