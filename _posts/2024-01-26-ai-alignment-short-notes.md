---
title: AI对齐论文阅读杂记
description: notes for ai-alignment papers do not need to write a whole blog
tags: ai-alignment
category: AI
date: 2024-01-26
---
## 2017

1. [Arixv] Deep Reinforcement Learning from Human Preferences
   1. 在RLHF之前的使用人类偏好做奖励学习的工作，面对的是在线游戏等其他的强化学习问题，处理原来工作中奖励函数需要大量人类标注的问题
   2. 使用轨迹段的偏好作为反馈训练奖励模型，而不是整段轨迹的评分
   3. 整体方法和RLHF基本一致，使用奖励模型的评分训练策略模型，但使用策略模型生成的轨迹的片段对让人类判断偏好，并用偏好去训练奖励模型。
   4. 奖励模型在整个过程中可能发生变化，并且在训练过程中没有要求策略模型和开始时区别不能过大
   5. 了解到因为奖励模型可能非静态，所以才使用了更加鲁棒的策略梯度方法，并且使用了多个奖励函数进行聚合平均来作为奖励评分

## 2018

1. [Arxiv]Supervising strong learners by amplifying weak experts
   1. 解决任务目标过于复杂或困难，人类难以直接评估并给定明确标签或信号的问题
   2. 提出了迭代增强方法，通过让人类识别一系列有用的子问题来回答目标问题，使用机器学习系统计算每个子问题的子答案，并且让专家在看到子答案后决定如何目标回答。
   3. 训练过程包含四个部分，首先是让专家分解出若干个子问题，AI系统回答这些子问题，专家根据子问题回答决定目标问题答案。然后根据得到的问答对训练专家预测模型来学习专家的决策，让专家预测模型预测子问题和最终的回答。接着利用专家预测模型充当专家角色，分解问题并交由AI系统进行回答。最后利用这些问答对来训练AI系统。于是AI系统就可以回答这些一开始难以回答的问题，在下次增强时就可以以这些问题作为子问题单位。
   4. > 所以RLHF的结果模型可以拿来做这个专家模型？以及这个和MoE什么关系，MoE可以看作这个东西的自动化吗？

## 2019

1. [Arixv] Fine-Tuning Language Models from Human Preferences
   1. 感觉和Deep Reinforcement Learning from Human Preferences的唯一区别就是用大模型充当了奖励模型，上一篇是使用轨迹段对之间的偏好来代替了绝对的评分数据，这一篇是使用大模型代替了简单的线性函数

## 2020

1. [NeurIPS] Learning to summarize from human feedback
   1. 和前面rlhf的论文工作内容基本一致，和deep reinfercement learning from human preferences的区别在于整个过程离线完成，使用的value模型用了和策略模型不同的初始化，初始化为了奖励模型。
   2. 一个方法换问题发了四篇论文了这是，真是如意算盘敲的震天响啊，算盘珠子崩我脸上了都

## 2022

1. [Nature] Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do
   1. 阅读原因是综述论文中说论文指出Bert内部表示空间存在一定的道德倾向，想看下实验是怎么做的
   2. 论文中对于道德的定义使用了一种描述性意义，根据一个行为本身在一系列规则下的正确与否来评价行为的道德性。研究预训练的LM在何种程度上包含了对所做事情的是非曲直，即对人类道德规范的类似人类的偏见。
   3. 道德方向的实验目的是在语言模型的嵌入空间中寻找一个方向，以评估被编码为文本短语的行为的道德可接受性。
   4. 实验首先把行动表述为问题，把问题表述为道德规范，从而强调了道德的方向，然后使用多个提问模板让bert进行嵌入，并计算平均的句子嵌入向量。由于很难定义规范和非规范行动对，论文定义了积极、中性和消极行动的代表性集合，并假设最前的主成分描述了方向，或者首个主成分是道德方向m。
2. [Arxiv] Scaling Instruction-Finetuned Language Models
   1. 对于所示的所有三种模型大小，与不进行微调相比，多任务指令微调可大幅提高性能。但大部分改进来自使用最多282个任务。可能因为多任务指令微调的主要作用是让模型学习更好地表达预训练中已经知道的知识，超过282个任务并没有太大帮助。
   2. 将模型规模增加一个数量级可显着提高微调和非微调模型的性能。与没有微调的模型相比，指令微调的改进幅度似乎并没有减少，这表明指令微调可能对未来的模型仍然有意义。
   3. 联合微调可以显着提高CoT性能，同时保持非CoT任务的性能，从而使单个模型能够在所有评估中表现良好。在混合微调数据集中包含九个带有思维链(CoT)注释的数据集可提高推理能力

## 2023

1. [Arxiv] Trustworthy LLMs: A Survey and Guideline for Evaluating Large Language Models’ Alignment
   1. LLM对齐的目标是构建一个可信任的LLM，文章将对齐的角度分为可靠性、安全性、公平性、误用的抵抗力、可解释性、社会规范和鲁棒性等七类，对每一类的对齐内容进行描述，相关工作提了一点。最后文章提出了自己设计的对齐程度的测量方法，主要思路两点：使用选择题代替问答题从而让答案可以被量化统计，使用被良好对齐的LLM来为不可量化的回答打标记，避免人类参与，提升测量效率。带来的问题就是测量方法高度依赖LLM对于输出格式的理解和处理能力，以及对齐程度受做测量工具使用的LLM的对齐程度限制。*本质上只能算二级度量，毕竟做测量工具使用的那个LLM肯定不会是用这个方法度量出来的。*
   2. 选择题不一定可以做出合逻辑的回答
   3. 使用知识库构建知识图谱，然后用知识图谱的表征学习方法对词嵌入向量进行调整？那如何设计
2. [Arxiv] Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models
   1. 针对原来的训练方法都需要额外的偏好标注数据集的问题，设计了新的训练方法，利用sft阶段的数据集进行对齐
   2. 基本逻辑其实有点对抗优化：首先训练评分模型来区分sft回答与LLM生成回答，然后训练LLM来生成评分模型更加难以分辨的回答。但最后使用LLM来表示了评分模型，将两个环节统一，做成了self-play形式（中间的推理过程和分析看明白了扩展成一篇博客）
   3. 最终的损失函数与DPO基本一致，只是reference model是上一轮训练的模型，好回答使用sft数据，坏回答由训练的LLM自行生成。
   4. > 不知道这样的reference会不会导致分布偏移问题，以及如果sft数据更强调专业性而不强调人类意图不就歇逼了？
3. [Arxiv] RRHF: Rank Responses to Align Language Models with Human Feedback without tears
   1. 解决RLHF训练对于参数敏感且需要四个模型，显存消耗高难以延展到大参数量模型的问题
   2. 采用不同的策略为同一个prompt采样若干个回复，然后使用奖励模型为每个回复进行评分，使用评分进行排序，最后使用评分损失优化策略模型对于好坏问答对的归一化对数概率的分差，给好回答更高的分值，给坏回答更低的分值
   3. 同时要求模型学习最优的回答，同步添加了一个监督微调的损失函数，将最优回答的对数概率也作为损失，和评分损失联合训练
4. [Arxiv] R-Tuning: Teaching Large Language Models to Refuse Unknown Questions
   1. 其实算幻觉问题和指令微调问题，通过鉴别模型参数化知识和训练数据知识之间的区别来提升回答确定性
   2. 使用in-context learning来判断模型是不是拥有对应问题的参数化知识，然后将训练数据分成确定和不确定两组，使用问题模板在正常的回答之后加问一句是否根据内部知识精确回答，并根据确定性添加是否回答。最后使用添加后的数据进行指令微调
   3. 看代码是没区分太明确，原来的问题还是问题，在回答这里直接添加了后面的第二问和回答，然后就拿去训练了
5. [Arxiv] Deep Reinforcement Learning from Human Preferences
   1. 在RLHF之前的使用人类偏好做奖励学习的工作，面对的是在线游戏等其他的强化学习问题，处理原来工作中奖励函数需要大量人类标注的问题
   2. 使用轨迹段的偏好作为反馈训练奖励模型，而不是整段轨迹的评分
   3. 整体方法和RLHF基本一致，使用奖励模型的评分训练策略模型，但使用策略模型生成的轨迹的片段对让人类判断偏好，并用偏好去训练奖励模型。
   4. 奖励模型在整个过程中可能发生变化，并且在训练过程中没有要求策略模型和开始时区别不能过大
   5. 了解到因为奖励模型可能非静态，所以才使用了更加鲁棒的策略梯度方法，并且使用了多个奖励函数进行聚合平均来作为奖励评分
6. [Arxiv] Fine-Tuning Language Models from Human Preferences
   1. 感觉和上一篇的唯一区别就是用大模型充当了奖励模型，上一篇是使用轨迹段对之间的偏好来代替了绝对的评分数据，这一篇是使用大模型代替了简单的线性函数
7. [ICLR] Self-Consistency Improves Chain of Thought Reasoning in Language Models
   1. 处理贪婪解码的重复性和局部最优性问题，以及单次采样生成的随机性问题
   2. 根据直觉认为一个复杂的推理问题通常会有多种不同的思维方式导致其唯一正确的答案，使用大模型使用多种方式生成回答，然后找到一致性最强（出现次数最多的）
   3. 首先用CoT prompt提示大模型，让大模型生成思考过程。然后在解码处采样一个候选输出的集合，以及对应的候选推理路径集合。最后将按答案聚合不同的候选路径，并选择一致性最强的答案作为最终答案。
8. [ICLR] Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization
   1. 大模型价值观对齐工作，针对大模型生成的毒性内容和社会偏见内容纠正问题
   2. 提出了一个统一的框架，同时完成毒性内容和社会偏见内容纠正。在生成的过程中对模型进行进一步微调，促使大模型生成时偏向选择无毒和无偏见的token
9. [Arxiv] R-Tuning: Teaching Large Language Models to Refuse Unknown Questions
   1. 其实算幻觉问题和指令微调问题，通过鉴别模型参数化知识和训练数据知识之间的区别来提升回答确定性
   2. 使用in-context learning来判断模型是不是拥有对应问题的参数化知识，然后将训练数据分成确定和不确定两组，使用问题模板在正常的回答之后加问一句是否根据内部知识精确回答，并根据确定性添加是否回答。最后使用添加后的数据进行指令微调
   3. 看代码是没区分太明确，原来的问题还是问题，在回答这里直接添加了后面的第二问和回答，然后就拿去训练了
10. [Arxiv]Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases
   1. 探索数据规模对指令微调的影响
   2. 翻译了英文的指令微调数据，并进行价值观修改。然后利用这些数据作为seed，使用icl让ChatGPT生成其他指令数据
   3. 对于翻译、重写、生成和头脑风暴任务，200万甚至更少的数据量就可以使模型表现良好。
   4. 对于提取、分类、封闭式问答和摘要任务，模型的性能可以随着数据量的增加而不断提高，
   5. 模型在数学、代码和CoT指令上的表现仍然较差，在数据质量、模型规模和训练策略方面还需要进一步探索。
11. [Arxiv]Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning
   1. 探索数据规模对指令微调的影响，限制数据规模的主要思路是减少指令的种类，专注于专门的任务。
   2. 方法思路是从已有的数据中检索出最有用的核心样本，帮助模型学习下游任务所需的知识。首先通过大模型将训练数据映射到嵌入空间，然后在嵌入空间进行k-means聚类+余弦相似度选最接近任务样本，最后使用KCentergreedy方法检索每个任务的最相似样本作为训练数据
   3. 实验结果发现对于指定任务的模型可能会从固定的任务类型中获益，指令的多样性对于指定任务的影响较小，少量的数据就可以在指定任务的模型上达到很好的微调效果
12. [Arxiv]Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning
   1. 对指令微调原理的分析，通过分析使用原本/改动的指令训练的模型表现来分析模型如何使用指令
   2. 指令微调目标主要分为向未知任务泛化和向未知指令泛化两种情况。向未知任务泛化方法在训练时使用一个任务集合中的指令微调数据，然后在测试时使用未知任务测试效果。向未知指令泛化方法在训练时没有清晰的任务和指令边界，在测试时使用手动构建的指令进行测试。本文主要关注向未知任务泛化的情况。
   3. 提出了两种指令改动方法：对于任务定义，通过移除指令中的所有语义部分，只保留输出空间中的信息创造了定义简化版本；对于任务示例，通过采样错误的输入输出映射关系创造虚假示例版本。
   4. 实验结果发现在简化任务定义和虚假样本上训练的模型可以获得在原有指令和示例上训练的模型类似的效果。随机猜测的baseline在训练样本受限时可以获得跟指令微调类似的准确度，说明当前IT模型令人印象深刻的性能增益可以来自捕获表面模式，如学习输出格式和猜测，而不是学习指令内部蕴含的逻辑。

## 2024

1. [Arxiv] Self-Rewarding Language Models
   1. 尝试解决需要超人类反馈提供训练信号的超人类智能体的对齐问题。提出了自奖励LM，通过LLM-as-a-Judge prompting使用LLM自身作为奖励模型。自己生成对齐的数据进行训练。
   2. 方法旨在能够同时拥有指令遵循和自指令生成两个能力的模型，可以在给定用户请求后生成高质量的无害回答，同时可以生成并评估新的指令遵循样本并添加到训练集中。
   3. 方法使用一组指令微调数据（指令问答对）和一组评估微调数据（评估指令问答对）进行监督微调，然后使用AI反馈（自反馈）数据进行增强，迭代完成训练。每次迭代通过IFT和EFT进行SFT后进行两轮的AIFT数据生成和微调训练。
   4. AIFT的生成：首先从IFT数据中采样prompt，使用few-shot prompting生成新的prompt，然后利用新的prompt生成若干个回答，最后使用LLM的LLM-as-a-Judge能力对每个回答进行评分。AIFT数据可以是分数最高和最低的回答构成的好坏问答对，也可以是满分回答构成的gold问答数据。
   5. 每轮训练流程包括一轮SFT和两轮DPO，每一轮都使用前一轮训练得到的模型作为基座模型，SFT和DPO训练完成后都用模型生成一版AIFT供下一轮训练
   6. 迭代次数的缩放效应，奖励入侵问题，安全性评估以及安全训练方法，以及这种训练方式是否可以用于提升模型安全性
2. [Arxiv] Fine-Tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!
   1. 大模型微调过程的安全性问题
   2. 已有安全基础设施主要围绕在模型内部嵌入安全规则，以限制推断时的有害行为，但在用户有微调权限时无法进行限制。
   3. 攻击场景是攻击者可以访问目标LLM进行微调，可以是访问开源模型权重也可以是调用微调API，攻击者会对抗性地设计数据点进行微调，以诱导初始对齐模型的恶意更改，使用供应商推荐/执行的默认微调算法。攻击者旨在攻破模型，拆除其安全护栏，使行为不受安全规则的约束。
   4. 将风险分为三级：使用明确有害的数据进行微调（有害实例演示攻击），使用隐含的有害数据进行微调（身份转换攻击），使用完全无害数据进行微调。
   5. 通过实验验证使用少量的有害数据进行微调就可以危害大模型使用大量数据进行良好对齐的效果。即使没有恶意的意图，简单地使用良性和常用的数据集进行微调也会无意中降低LLMs的安全对齐度，尽管程度较小。通过无害数据微调LLM使其自我身份识别为完全遵循用户指示，可以绕过LLM的数据调整机制，使LLM输出与对齐目标不一致的高风险回答。
   6. 预训练和对齐的策略虽然资源密集，但并不能完全阻止"越狱"。尽管模型主要是在合适的情境中训练的，但模型仍然可以学习概括导致有害行为的出现或"幻觉"。微调数据调整依赖于调整的精度，更隐式的身份转移数据没有被测试的中的任何一个数据调节系统标记，说明仅靠数据调整可能不足以解决所有的安全问题。在微调中给微调数据中混入一定量的安全数据可以提升对其效果，闭源的模型微调API可以将用户的定制数据与安全数据强制性混合在一起，而开源社区可以考虑开发默认混合了安全数据的更安全的训练器，其他方法包括正则化微调或连续学习方法。
   7. 微调后，安全审计可以利用全面收集的有害指令(例如,制定政策导向的基准)通过自动化的红队测试进行。技术缓解策略可以与政策干预相结合，例如要求某些微调的安全机制作为负责任使用许可证的一部分。
3. [Arxiv] Tradeoffs Between Alignment And Helpfulness In Language Models
   1. 研究特征工程对齐的有效性和对齐性之间的权衡问题。
   2. 特征工程对齐指对模型隐层表征添加注入表征修改分布达到对齐目标的方式
   3. 文中给出了注入表征后行为分数的下界，证明对齐效果与注入的表征范数成正比例线性增长。同时也给出了有用性（给出正确回答）的上界，证明有用性与注入表征的范数的二次方成反比，成抛物线下降趋势
   4. > 具体的证明没有细看，但文中没有给出应该怎么用这个效果，没有提出新的注入方法
4. [Arxiv] Using Hallucinations to Bypass RLHF Filters
   1. LLM安全问题，使用大模型的幻觉能力绕过安全对齐的限制，算一种越狱攻击？
   2. 使用翻转文本绕过大模型对于敏感内容的筛选，然后要求大模型翻转不存在的自然段的内容，主动引发大模型的幻觉，让大模型自己完成翻转后生成敏感内容
   3. > 对文心一言也有效，但这个是怎么个原理，对齐到底对齐了什么
   4. 论文中指出RLHF其实非常浅层，只能让文本预测在特定的"环境"中进行，即一个具有受控风格和"个性"的环境。通过逆转文本让文本生成在rlhf没有控制的环境中进行
   5. > 所以存在一个对于rlhf的控制环境的探索？也是稳定性吧，覆盖程度？
5. [ICLR] Safe RLHF: Safe Reinforcement Learning From Human Feedback
   1. 解决RLHF的"有用性"和"无害性"目标之间的权衡问题，有用的回答可能有害，无害的回答可能没用
   2. 已有的大部分方法仅优化一个单一目标，可能导致模型优化到次优结果。一些方法细化了不同的属性，使用不同的奖励模型进行匹配，然后引导LLMs进行微调，以确保模型集成多种属性。这种方法需要手动调整奖励和成本之间的权重(类似于奖励塑造)，难以直接应用在多种场景中。
   3. 本文提出了一种新的框架safe RLHF，Safe RLHF的核心见解是在数据标注过程中对人类偏好进行解耦，并建立两个优化目标：有用性和无害性。在数据标注过程中，它确保了众包工作者的反馈在有用性和无害性之间保持无偏见；在Safe RLHF阶段，拉格朗日方法可以自适应地平衡两个固有冲突的训练目标之间的权衡。
   4. 在数据标注阶段提出了一种两步标注方法，给标注者同一个prompt的两个回答，要求分别对有用性和有害性进行打分。有用性只用给出偏好评分，有害性除了对有害性进行评分外还要对每个回答给出绝对性的标签，标注是有害回答（+1）还是无害回答（-1）（那数据不平均的问题怎么解决）
   5. 在偏好模型这里独立训练两个模型，奖励模型和RLHF一致，使用偏好数据度量回答的有用性偏好。成本模型使用有害性偏好标注度量有害性偏好（方式和奖励模型一致），再加上有害性真实标签的度量，使有害和无害样本的成本函数差距尽可能大。
   6. 在强化学习阶段使用拉格朗日方法将成本函数和奖励函数结合到一起，然后使用PPO算法进行训练
   7. > 又多了一个模型，这一共是policy+reference+reward+cost一共四个模型了，训练资源要求更高了
6. [ICLR] Statistical Rejection Sampling Improves Preference Optimization
   1. 解决DPO和SLiC等offline方法的偏好对采样问题
   2. DPO中奖励模型的缺失限制了它从最优策略中采样偏好对的能力。SLiC只能从SFT策略中采样偏好对。
   3. 方法采用SFT策略模型、奖励排名模型和prompt作为输入。首先通过拒绝抽样方法对最优策略的响应进行采样，然后在标记的偏好对上拟合分类模型。在损失函数的选择上根据偏好数据训练二值分类器，对策略概率进行归一化，得到DPO的SVM变体作为归一化似然的铰链损失。在偏好数据的分布上先训练一个上面提到的reward ranking model，用其推导原始奖励函数，进而推导出policy模型，从这个policy模型上采样response并用奖励评分模型标注偏好得到偏好数据集。拒绝采样方法上使用SFT模型生成回答，并交由评分函数进行评分，最后根据评分和高斯分布采样的值的大小来决定样本是否采用
   4. 首先使用现有的preference data，训练一个pair-wise的reward ranking model，然后使用统计拒绝采样算法，通过这个reward model和SFT policy来近似地生成optimal policy的序列pair。给这些序列对用reward model打标后就可以套用分类loss来训练policy了。
7. [EACL] Gradient-Based Language Model Red Teaming
   1. 大模型对齐效果的红队测试问题，提出了一种新的自动化红队测试样例生成方法
   2. 对齐过程的有效性在很大程度上依赖于多样化的提示，这些提示可以触发模型生成低安全分数的响应。已有方法有劳动密集的问题，这限制了红队测试prompt的规模和多样性。
   3. 提出的方法使用LM生成prompt，包括一个语言模型，一个分类器，分类器负责为语言模型输出结果的安全性打分，最后根据分数来优化输入语言模型的prompt。
   4. 为解决输入和生成回答的采样策略导致的不可微，进而导致无法通过反向传播优化的问题，文中引入Gumbel softmax trick，给每个token加上一个gumbel分布，然后进行softmax。
   5. 每个生成的token都由prompt和已经生成的token决定（应该是靠transformers decoder的mask-attention完成）
   6. 为了鼓励找到更有意义的prompt，添加了一个真实损失正则$$-\sigma(x)*x'$$来惩罚prompt分布和预训练模型分布的差异（这个没看明白，说是LM根据前一个prompt token来预测下一个可能的token，但预测的是回答又不是prompt）
8. [Arxiv] Learning or Self-aligning? Rethinking Instruction Fine-tuning得知了两个结论
   1. 对于指令微调而言，学习与模型参数知识不一致的世界知识无法带来增益，甚至会造成额外的损害。
   2. 有效指令微调的本质在于完成行为模式转换的同时，保持指令微调前后模型参数知识的一致性。
   3. 换句话说，指令微调的核心作用机制并不是让模型去“学习”额外的知识，而是将模型内部现有的知识进行一种自我的对齐。因此，最终决定指令微调性能的并非领域知识的注入程度，而是是否能够通过指令微调的过程，实现更有效的自我对齐，从而促进模型内部现存的参数知识在零样本问答这一目标模式下更好地表达。