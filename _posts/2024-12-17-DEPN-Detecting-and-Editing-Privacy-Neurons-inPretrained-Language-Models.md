---
title: DEPN方法论文阅读笔记
description: notes for llm unlearning paper ‘DEPN-Detecting and Editing Privacy Neurons in Pretrained Language  Models’
tags:
  - ai-explanation
  - llm-unlearning
category: AI-safety llm
date: 2024-12-17
---
在大量数据上进行预训练的大型语言模型可以捕获训练数据中的丰富知识和信息。先前的研究揭示了预训练语言模型中数据记忆和反省的能力，这带来了数据泄露的风险。为了有效降低这些风险，本文提出了一个框架DEPN来检测和编辑预训练语言模型中的隐私神经元，部分灵感来自知识神经元和模型编辑。在DEPN中，本文引入了一种新方法，称为隐私神经元检测器，用于定位与隐私信息相关的神经元，然后通过将其激活设置为零来编辑这些检测到的隐私神经元。此外，本文提出了一种隐私神经元聚合器，以批处理方式取消记忆隐私信息。实验结果表明，DEPN可以显着有效地减少隐私数据泄露的风险，而不会降低模型的性能。此外，本文从多个角度（包括模型大小、训练时间、提示、隐私神经元分布）实证证明了模型记忆与隐私神经元之间的关系，说明了本文方法的鲁棒性。

## 本文的背景

近年来，大型语言模型 (LLM) 取得了显著进展。然而，尽管取得了成功，LLM 在实际应用中仍面临隐私和安全问题。隐私和安全风险的主要原因是大型预训练语言模型的固有性质。先前的研究表明，预训练语言模型倾向于记忆和重复很大一部分训练数据，包括在训练数据中仅出现一次的非典型数据点。此外，外部因素（例如成员身份攻击）也加剧了这些风险。人们已经探索了多种方法来攻击 LLM 以提取训练数据。例如，Carlini 等人(2021 年)成功地从 GPT-3 的输出中提取了个人信息，而 Li 等人（2023 年）则利用 ChatGPT 中的多步骤提示诱导生成个人信息。所有这些都表明，大型预训练语言模型面临着严重的隐私泄露风险。

大多数保护隐私的方法都侧重于在数据处理阶段删除敏感信息，或者在训练阶段减少模型记忆训练数据的程度。然而，隐私泄露往往在模型训练完成后才被发现，这使得以前的方法效果不佳。在后处理阶段也提出了一些方法，这些方法涉及轻微的参数再训练，使模型忘记隐私信息。然而，这些方法通常会导致较高的计算复杂度，因此很难将它们应用于复杂的模型架构。在实践中，模型开发人员经常尝试通过阻止或过滤某些关键字来阻止语言模型输出特定信息，但这并不能真正解决根本问题。

## 本文研究的问题

将模型编辑技术引入大模型遗忘问题。在预训练模型上，从模型结构角度设计高效灵活的隐私遗忘方法，移除模型中包含的隐私信息。

## 已有方法为什么不行

**模型编辑**方法可以分为四种策略。约束微调策略专门针对目标只是更新LLM，从而实现精确修改。基于记忆的编辑策略维护一个知识缓存，其中存储新信息以替换不良预测。基于元学习的编辑策略引入基于元学习的可编辑训练，训练模型参数以适应编辑。定位和编辑策略假设知识本地存储在 LLM 中，定位与知识相关的特定参数并直接编辑参数以执行编辑。定位的基本思想是改变神经元的参数，然后观察模型预测的对象实体的概率变化。然而这种方法一次只能观察一个token的概率变化，但语义单元通常由一系列token组成。

**隐私保护**可分为三个主要应用阶段：数据处理阶段、预训练和/或微调阶段以及后处理阶段。在数据处理阶段，方法涉及删除或替换原始数据中的敏感信息。在预训练或微调阶段，可以通过修改模型训练过程来保护数据隐私。一种方法是差分隐私随机梯度下降 (DPSGD)，在截断梯度中引入噪声以减少梯度之间的区别并防止记忆训练数据。另一种方法是对抗训练，通过对抗训练技术限制模型对隐私信息的学习。但是，**如果在模型训练完成后才发现隐私泄露，则数据处理阶段和预训练或微调阶段使用的方法不适用**。后处理阶段使用的方法侧重于让训练好的模型忘记特定数据或更改特定参数以保护隐藏的私人信息。这些方法通常**计算成本高，不易应用于大型模型**。相比之下，提出的 DEPN 可以在后处理阶段以较小的计算开销实现对私人信息的保护。

## 本文的方法

本文对隐私的定义遵循狭义和广义两种，隐私的狭义定义将个人身份信息视为隐私，例如姓名、身份证号码、电话号码和其他相关信息。隐私的广义定义将文本中与个人相关的任何信息都视为隐私。

本文方法包含三个组件：隐私神经检测器，隐私神经编辑器和隐私神经聚合器。

**隐私预测任务**

给定大模型$$\theta$$和文本$$T={X,Y}$$，其中$$X$$是有关的上下文，$$Y={y_1,...,y_n}$$是包含隐私信息的token序列。大模型泄露隐私序列的概率就是在输入上下文时生成对应序列的概率。

$$
P(Y\vert X,\theta)=\Pi_{i=1}^{\vert Y\vert}P(y_i\vert X,\theta)
$$

比如，训练数据可能包含了XX是ESPN的写手的信息，对于模型的一个输入‘_ _ 是ESPN的写手’，本文目标是避免生成对应的人名XX。

**隐私神经检测器**

本文遵循的预想是事实知识以键值存储器的形式存储在Transformer的前馈网络中。因此本文推定隐私信息可能也编码在特定的神经元中。本文提出了一种基于梯度积分的隐私归隐方法，用于评估那些神经元在语言模型隐私信息泄露中起关键作用。

假设$$w_l^k$$是$$l$$层的前馈网络中的第$$k$$个神经元，前面的隐私泄露概率可以描述为神经元$$w_l^k$$是特定值的时候的隐私序列生成概率

$$
P(Y\vert X,\theta)=\Pi_{i=1}^{\vert Y\vert}P(y_i\vert X,w_l^k=\alpha_l^k)
$$

本文将目标神经元的参数从0逐渐变化到神经元的原始值，然后观察目标序列的生成概率变化。本文计算此过程中概率变化的累积梯度，作为神经元对隐私敏感输出的贡献（即隐私归因分数），即

$$
Att(w_l^k)=\beta_l^k\int_0^{\beta_l^k}\frac{\partial P(Y\vert X,\alpha_l^k)}{\partial w_l^k}d\alpha_l^k
$$

本文使用Rienmann近似，因为难以直接计算连续积分，本文设定近似步数$$m=20$$。

$$
Att(w_l^k)=\frac{\beta_l^k}{m}\sum_{j=1}^m\frac{\partial P(Y\vert X,\frac{j}{m}\beta_l^k)}{\partial w_l^k}
$$

再考虑前面的序列生成概率是单个token生成概率的积（自回归生成过程），可以把这个分数计算为

$$
Att(w_l^k)=\sum_{i=1}^{\vert Y\vert}\frac{\beta_l^k}{m}\sum_{j=1}^m\frac{\partial P(Y\vert X,\frac{j}{m}\beta_l^k)}{\partial w_l^k}
$$

如果神经元对隐私信息的输出影响较大，则梯度较大，因而会得到更大的积分值。所以隐私归因得分可以衡量神经元对隐私信息泄露的贡献，隐私归因得分越大，神经元的隐私敏感度越大。我们选取隐私归因得分排名前$$z$$的神经元作为编辑候选。

**隐私神经编辑器**

就是把这些神经元的值归0

**隐私神经聚合器**

当输入为文本批次时，本文计算批次中每个序列的隐私归因得分矩阵。在计算隐私归因得分后，本文让每个序列根据其隐私归因得分为神经元投票，并选择得票最多的前$$z$$个神经元。这些选定的神经元将被编辑以删除隐私信息。超参数z根据模型大小、训练期和其他因素进行调整。

> 也就是说选择隐私得分总和最高的前$$z$$个神经元进行编辑。

## 本文如何说明有效性

## 可能的未来方向

这些隐私神经元负责记忆隐私信息并不代表这些神经元只负责记忆隐私信息，应该不能直接归0，这应该也是后面一些工作说这个方法会导致模型可用性退化的原因。