---
title: DPO背景知识
description: notes for "Direct Perference Optimization：Your Language Model is Secretly a Reward Model"
tags: ai-alignment llm
category: AI
date: 2023-12-18

# bibliography: 2023-11-02-xai.bib
---
## RLHF流程回顾

**SFT**：RLHF首先从在下游任务的优质数据上微调一个预训练LM开始，得到监督微调模型$$\pi^{SFT}$$

**Reward Modelling Phase**：在第二个阶段，SFT模型得到提示词并生成一对回答，这对回答交由人工标记偏好。人工的偏好被认为是遵循某个我们不知道的奖励模型生成的，Bradley-Terry模型用于建模这种偏好，将人类偏好分布描述为

$$
p*(y_1\prec y_2\vert x)=\frac{\exp(r*(x,y_1))}{\exp(r*(x,y_1))+\exp(r*(x,y_2))}
$$

假设可以接触到一个从分布中采样的静态数据集$$\mathcal{D}=\{x^{(i)},y_w^{(i)},y_l^{(i)}\}^N_{i=1}$$，就可以参数化一个奖励模型$$r_{\phi}(x,y)$$，并用最大似然来估计参数，最终得到损失函数

$$
\mathcal{L}_R(r_{\phi},\mathcal{D})=-\mathbb{E}_{(x,y_w,y_l)\sim D}\left[\log\sigma(r_{\phi}(x,y_w),r_{\phi}(x,y_l))\right]
$$

在大语言模型环境中，奖励模型一般会用SFT模型加一个输出单一分数的线性层组成。为了保证奖励函数有低方差，已有工作会标准化奖励分数，保证期望为0。

**RL Fine-Tuning Phase**：在RL阶段会使用奖励模型来提供环境反馈，使用PPO方法来优化如下问题

$$
\max_{\pi_{\theta}}\mathbb{E}_{x\sim D,y\sim\pi_{\theta}(y\vert x)}\left[r_{\phi}(x,y)\right]-\beta\mathbb{D}_{KL}\left[\pi_{\theta}(y\vert x)\vert\vert\pi_{ref}(y\vert x)\right]
$$

## DPO

DPO方法可以利用奖励模型参数化的特定选择，能够以封闭性是提取最佳策略而无需RL训练循环。关键见解是利用从奖励函数到最优策略的分析映射，从而将奖励模型的损失函数转换为策略模型的损失函数的一部分。

首先根据上面的损失函数，（论文说）可以计算出在KL约束下的最优解是

$$
\pi_r(x\vert x)=\frac{1}{Z(x)}\pi_{ref}(y\Vert x)\exp(\frac{1}{\beta}r(x,y))
$$

然后做转换把$$r(x,y)$$提出来，得到

$$
r(x,y)=\beta\log\frac{\pi_r(y\vert x)}{\pi_{ref}(y\vert x)}+\beta\log Z(x)
$$

因为Bradley-Terry模型仅依赖于$$r(x,y)$$，即$$p*(y_1\prec y_2\vert x)=\sigma(r*(x,y_1)-r*(x,y_2))$$。代入上式可以得到

$$
p*(y_1\prec y_2\vert x)=\frac{1}{1+\exp(\beta\frac{\pi*(y_x\vert x)}{\pi_{ref}(y_2\Vert x)}-\beta\frac{\pi*(y_1\vert x)}{\pi_{ref}(y_1\vert x)})}
$$

于是概率与奖励模型没有关系了，因此可以根据refer model来直接优化policy model，于是损失函数变为

$$
\mathcal{L}_{DPO}(\pi_{\theta};\pi_{ref})=-\mathbb{E}_{(x,y_w,y_l)\sim D}\left[\log\sigma(\beta\log\frac{\pi_{\theta}(y_w\vert x)}{\pi_{ref}(y_w\Vert x)}-\beta\log\frac{\pi_{\theta}(y_l\vert x)}{\pi_{ref}(y_l\vert x)})\right]
$$

> 理论分析先略过了

