---
title: AI explanation论文阅读杂记
description: notes for ai explanation papers do not need to write a whole blog
tags: ai-explanation
category: AI
date: 2024-01-26
---
## USENIX Security

### 2023

1. AIRS: Explanation for Deep Reinforcement Learning based Security Applications
   1. 深度强化学习方法的可解释性问题，设计了一个新的模型来预测每个状态对于强化学习最终奖励的影响程度
   2. 已有工作的问题：主要关注于特征级别的解释，在给定步骤解释动作，无法解释在整体任务上的成败；使用价值函数评估状态重要性无法为DRL提供高准确度的解释，且价值函数不是一直可获得；粗粒度的全局解释无法解释单一的一次运行；现有工作不关注解释的稳定性和有效性
   3. 方法假设可以获知DRL方法的环境信息（输入）和获得的最终奖励。方法不需要接触价值函数和policy模型的参数，可以将模型视为黑盒。
   4. 方法包含一个RNN和两个DNN，RNN用于将输入转换为时序编码，保留时序信息。降维DNN用于捕获时序编码中的高阶信息，并完成降维。因子DNN根据时序编码计算状态的线性回归系数，作为状态的重要性。
   5. 联合训练和真实最终奖励的最大均方差，以及一个正则项保证类似状态预测的平滑性。完成后过滤出得到与每个状态序列相关性最低的状态，视作不重要状态。预测时输入状态序列特征，RNN计算状态编码，使用因子DNN计算重要程度，如果不是不重要状态则返回，如果是则归0.
   6. > 这玩意真能解释吗...它真的知道自己在解释什么吗。严格来说它在分析结果，而不是分析原因。它的重要性依赖于这个模型的状态动作转移真的有分布（这个好像真的有），它的解释含义应当是“如此如此转移后，这个状态对于最终结果的影响是最大的”，并以此认为对应的DRL模型也是因此作出的这个决策。