---
title: 哈利波特遗忘论文阅读笔记
description: notes for llm unlearning paper ‘whos harry potter? approximate unlearning in llms’
tags:
  - ai-explanation
  - llm-unlearning
category: AI
date: 2024-12-08
---
大型语言模型 (LLM) 在海量互联网语料库上进行训练，这些语料库通常包含受版权保护的内容。这对这些模型的开发者和用户以及原作者和出版商构成了法律和道德挑战。本文提出了一种遗忘技术，用于从 LLM 中遗忘训练数据的子集，而无需从头开始重新训练。
本文在从 Llama2-7b 模型遗忘哈利波特书籍的任务上评估了我们的技术。在大约 1 GPU 小时的微调中，本文有效地消除了模型生成或调用哈利波特相关内容的能力，而其在常见基准测试上的表现几乎不受影响。
由三个主要部分组成：首先使用一个在目标数据上进一步训练的强化模型，通过将其逻辑与基线模型的逻辑进行比较，识别与目标最相关的标记。其次用通用的对应词替换目标数据中的特殊表达，并利用模型自身的预测为每个标记生成替代标签。这些标签旨在近似未在目标数据上训练的模型的下一个标记预测。在这些替代标签上对模型进行微调，这样每当模型被提示上下文时，就会有效地从模型的内存中删除原始文本。

## 本文的背景
在快速发展的人工智能和机器学习领域，大型语言模型 (LLM)的出现带来了全新的机遇和挑战。这些模型经过大量文本数据的训练，包含了大量的人类知识、语言模式和文化差异。然而，它们的庞大性和全面性也带来了许多伦理、法律和技术问题。训练大模型的语料库往往包含有问题的内容。这可能包括受版权保护的文本、有毒或恶意数据、不准确或虚假内容、个人数据等。当LLM复制、回忆这些文本，甚至受到这些文本的启发时会带来道德、法律和技术上的风险。

## 本文要研究的问题
在LLM训练并发布后，如何选择性地遗忘一个训练数据的子集

## 已有方法为什么不行
（当时的）已有方法主要侧重于通过基本的微调来添加或强化知识，但不提供“忘记”或“忘记”知识的直接机制。直接重训模型的时间和资源成本过高。

（当时的）遗忘方法主要关注分类任务，对生成模型和特定LLM的关注比较少。[[2024-12-02-Knowledge unlearning for mitigating privacy risks in language models]]主要关注降低隐私风险，kga需要知道原来的训练数据。

简单的梯度上升可能会破坏模型对于语言的理解能力，因为token并不只出现在版权保护内容中。此外简单进行梯度上升可能导致预测到版权保护的其他内容，整体而言没达到遗忘效果。

## 本文方法是什么

## 本文怎么说明效果的

## 可能的未来方向