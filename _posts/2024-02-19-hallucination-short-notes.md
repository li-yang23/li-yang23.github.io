---
title: 幻觉论文阅读杂记
description: notes for hallucination papers do not need to write a whole blog
tags: hallucination short-notes
category: AI-safety llm
date: 2024-02-19
---
# 幻觉原因分析
## 2020

1. [ACL] On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation
   1. 暴露偏差对于神经机器遗忘的效果的影响，实验分析了暴露偏差与生成幻觉文本之间的关系
   2. 使用教师强迫的训练知识将模型暴露在黄金历史中，而在推断过程中先前的预测可能是错误的。所以使用教师强制训练的模型可能过度依赖先前预测的单词，这将加剧错误传播，出现一个token预测错导致后面的token全都预测错的现象
   3. 将翻译幻觉定义为翻译出还算流畅但与原文本意思不同的译文的现象。实验发现标签平滑在其他方面提高了翻译质量，而最小风险训练（MRT）对幻觉的数量有明显的影响。
   4. MRT倾向于在较后的时间步增加模型的确定性，参考译文的增长幅度比干扰译文的增长幅度大。
   5. **总的来说**，MRT修改了模型的训练目标，除了对数概率还引入了译文和标签的差异度，从而有效减轻了幻觉。说明**次优的训练目标也会导致模型出现幻觉**

## 2022

1. [ACL] How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis
   1. 预训练模型预测的关联性分析，使用不同的策略（位置相近，知识关联，高共现，随机关联）对句子中的次进行遮罩，然后使用填空准确率度量知识捕获程度。
   2. 当干扰位置相近的单词时，准确性下降最多，而干扰知识相关的单词时，准确性下降最少。扩大模型规模，使用额外的训练数据，或者改进掩蔽策略，趋势变化不大。说明预训练模型主要捕获的是位置的相关性，而不是知识的相关性
   3. 对KD关联的依赖程度与探测性能呈正相关。说明PLMs对知识依赖关联的依赖程度越高，越能捕捉到相应的事实性知识。过分依赖位置上的紧密关联对探测性能是有害的。

## 2023

1. [Arxiv] Impact of Co-occurrence on Factual Knowledge of Large Language Models
   1. LLM出现事实性错误的原因分析，假设严重依赖预训练语料的简单共现统计是造成事实错误的主要因素之一。
   2. 知识表示为主谓宾三元组，并转换为自然语言形式。然后将其中的宾语遮罩用于向LLM提问，能够正确填空说明LLM知道对应知识（大致实验方法）
   3. LLMs的事实探测准确性与主语-宾语共现高度相关，导致对罕见事实的回忆失败。扩大模型规模或微调模型可以提高事实性知识探测的整体表现，但并不能解决共现偏差问题。
   4. LLMs倾向于生成与主语共现频率足够高的词，预训练数据的共现统计可能作为虚假特征，引起幻觉。记忆对于回忆事实是必要的，因为测试集中的事实性知识可能无法根据训练集中其他事实的先验知识进行推断。
   5. 微调显著提高了事实性知识探测的准确性。LLMs难以学习在预训练语料中很少出现的事实，尽管它们是在微调过程中明确给出的。
   6. 在频繁事实上的性能显著下降，而在稀有事实上的性能改善随着更多的样本被过滤掉而微乎其微。无论共现次数如何，去偏模型和基线的性能都是相似的，这意味着去偏微调的效果是不可泛化的。
2. [Arxiv] Lost in the Middle: How Language Models Use Long Contexts
   1. 大模型对于长文本召回问题的研究
   2. 当改变相关信息的位置时，性能会显著下降，这表明当前的语言模型在长输入上下文中不能稳健地利用信息。当相关信息出现在输入上下文的开头或结尾时，性能往往是最高的，而当模型必须在长上下文的中间访问相关信息时，即使是显式的长上下文模型，性能也会显著下降。
   3. 实验(i)通过改变输入上下文(类似于在检索增强生成中检索更多或更少的文档)中文档的数量来控制输入上下文的长度，(ii)通过改变文档的顺序来控制相关信息在输入上下文中的位置，将相关文档放置在上下文的开头、中间或结尾处。
   4. 编码器-解码器模型对相关信息在其输入上下文中的位置变化具有较强的鲁棒性，但仅对其训练时间序列长度内的序列进行评估。查询感知的上下文化(将查询放在文档或语义相关对的前后)能够在合成键值任务上获得近乎完美的性能，但在多文档问答中的趋势变化很小。即使是基础语言模型(即无需指令微调的模型)，当我们改变相关信息在输入上下文中的位置时，也会显示出U型的性能曲线。
   5. 模型结构的影响上，编码器-解码器模型可以更好地利用它们的上下文窗口，因为它们的双向编码器允许在未来文档的上下文中处理每个文档，从而潜在地改善文档之间的相对重要性估计。
   6. 读者模型性能在检索器性能达到饱和之前很久就达到饱和，说明读者没有有效利用额外的上下文。对检索到的文献进行有效的重排序(将相关信息推向输入语境的起始位置)或排序列表截断（在适当的时候检索较少的文档）可能是改进基于语言模型的读者如何使用检索到的上下文的很有前途的方向。
3. [Arxiv] BATGPT: A Bidirectional Autoregressive Talker from Generative Pre-trained Transformer
   1. 一个新的预训练模型，使用了双向的自回归结构来捕获自然语言的复杂结构，降低模型的内存限制问题和幻觉问题
   2. 也就是综述论文里面提的单一方向的自回归方法对于复杂结构的捕获能力不强，导致出现幻觉
   3. 但更有意思的是使用的参数扩展和堆叠训练，从一个简单的模型开始，通过复制堆叠和参数扩展逐渐加码成一个大型的模型
4. [Arxiv] Exposing Attention Glitches with Flip-Flop Language Modeling
   1. 大模型输出事实不准确，表现出错误的推理的原因分析，分析了注意力毛刺，即Transformer架构的归纳偏差无法稳定捕获健壮推理的现象。
   2. 提出了触发器语言建模(FFLM)是一个参数化的合成基准家族，旨在探索神经语言模型的外推行为。让大模型根据对1bit数字的指令操作集合预测最终输出的（1bit）数字结果。
   3. Transformer不能稳健地外推到长程依赖的长尾部分，在多次迭代之后在fflm上的错误率不再下降
   4. 软注意力的缺陷在于它的softmax操作可能"太软" --对任何具有固定范数的权重矩阵，随着序列长度的增加，注意力在不同位置上被"稀释"，并且不能执行预定的"硬选择"操作。自注意力可以自信地关注错误的指标，除非权重矩阵恰好满足正交性条件。也就是**注意力毛刺会导致模型出现幻觉**
   5. 最有效的缓解操作是在更多样化的数据上进行训练，扩大资源规模有一定帮助。标准正则化和注意力锐化方法也有一定作用
   6. > 是否可以看看注意力毛刺和暴露偏差这些问题导致幻觉的位置，是影响了解码的推理过程还是影响了编码的过程？
5. [Arxiv] How Language Model Hallucinations Can Snowball
   1. 大模型推理的雪球现象导致的幻觉问题的实验研究，LLM对早期错误的过度承诺，导致反之不会出现的更多错误。
   2. 使用问答数据进行实验，回答部分首先要给出一个整体的判断（Yes/No），然后再给出解释。
   3. 一旦LM产生了"是"或"否"，这个标记就会留在语境中，而连贯性则需要通过随后的辩护来承诺这一选择。因为transformers必须用一个步骤来回答一个需要多个时间步才能正确回答的问题，但由于Transformer在一个时间步内的推理能力有限，无法在一个时间步内找到答案，所以它有时必然会承诺一个错误的答案。
   4. 结论与选择题的实验结论相符，与[幻觉的理论分析论文](https://li-yang23.github.io/blog/2024/hallucination-is-inevitable/)结论也相符。
   5. 开始的整体回答超出了单步推理的能力极限，容易引发错误推理，然后一开始的错误推理引发雪球幻觉，导致后续的推理出现更多的错误
   6. 幻觉雪球很可能是暴露偏倚的结果：LMs在训练期间只暴露于黄金历史，但在推断期间大模型可能是依据错误的先前预测进行推理。更强调让模型在生成答案之前产生*推理链*可能是一种很好的方法，以适应其计算限制，并避免致力于强迫幻觉的错误答案。
6. [ICML] Large Language Models Struggle to Learn Long-Tail Knowledge
   1. LLM记忆的知识与预训练数据中的信息之间的关系的实验研究
   2. 语言模型回答基于事实的问题的能力与在预训练过程中看到多少与该问题相关的文档有关：LLM记忆的知识与预训练数据中相关实体之间共现的频率高度相关，去除共现数据后LLM问答效果骤降。
   3. 模型必须按许多数量级进行缩放，以在预训练数据中很少支持的问题上达到有竞争力的QA性能：模型性能与模型规模呈对数线性关系，需要将模型扩大几个数量级才能进行高精度问答
   4. 检索增强可以有效提升模型在稀少共现的信息上的问答性能，单纯地扩大数据的规模和多样性没什么用处
7. [NeurIPS] Why Does ChatGPT Fall Short in Providing Truthful Answers?
   1. GPT提供真实答案方面的问题的实验分析，将问题分为问题理解与意图、事实性正确性、具体性水平、推理四类，简称理解错误、事实性错误、具体性错误、推理错误。
   2. 近一半的错误是由于真实性错误造成的，其次是推理错误、理解错误和具体性错误。提供证据不仅可以解决真实性问题，而且可以显著减少理解性和具体性错误。
   3. 外部知识粒度越细，效果越好，外部知识整合提升了表现，而有效性受知识粒度的影响。
   4. 提供相关的键值有助于回忆必要的知识，提供完整的实体名称提高了性能，而提供实体背景或定义句则进一步帮助知识回忆，即使没有必要的知识。
      1. 提供细粒度的外部语境作为证据，可以帮助记忆必要的知识，因为仍然有大量的知识在训练过程中难以覆盖或在推理过程中难以回忆。
      2. 提供实体的描述作为键值可以帮助回忆必要的知识。
8. [EACL] When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories
   1. LLM对事实性知识记忆能力和限制的实验研究（幻觉的召回失败分析）
   2. LMs难以回忆不太流行的事实性知识，在这些情况下，检索增强有明显帮助。scaling主要提高了流行知识的记忆，对长尾知识中事实性知识的记忆没有显著提高。
   3. LM的记忆往往局限于流行的事实性知识，对出现频率较低的实体进行有限的记忆。非参数记忆在很大程度上提高了模型在长尾分布上的表现，在主题实体不普遍的情况下，检索增强的LMs特别具有竞争力
   4. 即使不使用上下文实例，较大的LM也表现出合理的性能，说明大型LM在一定程度上记忆了其参数中的事实性知识。
   5. 对于几乎所有的关系类型，主题实体流行度与模型准确率之间都存在正相关关系，主题实体流行度可以作为LMs事实性知识记忆的可靠指标。
   6. 检索增强LMs比无辅助的LMs具有明显的优势，特别是在不太受欢迎的实体上，产生了显著的性能增益。在比较流行的实体上，参数化知识可以获得可比较的准确度，说明LLM已经记忆了答案，带有检索上下文的增添输入对成绩的帮助不大，甚至有损成绩。
   7. 最后提出了一个检索增强方法，使用一个流行度阈值来决定是否进行检索。由于目前最好的LM已经记住了更多的流行知识，只有当LMs没有记住事实性知识，从而需要寻找外部非参数知识时才使用检索。方法有效增强了准确度表现，并且降低了推理成本。

## 2024

1. [Arxiv] Simple synthetic data reduces sycophancy in large language models
   1. 大模型奉承现象的分析和研究，使用合成的数据来避免奉承用户意见
   2. 主要两个结论是模型缩放和指令微调都显著提高了模型的奉承现象，以及模型即使知道用户的观点不正确，也能表现出奉承现象
   3. 提出了一种使用人工合成数据进行干扰的方式对模型进行微调，生成一些真实性与用户的意见无关的声明，并与其他指令微调数据混合进行微调，实验结果发现可以有效降低奉承现象
   4. 论文假设如果模型不知道知识的话就无法学习到“声明的真实性与用户意见无关”这个规则，所以生成的时候会先把用户意见去掉过一遍模型，用来检查模型是否具有对应的知识，然后将知识未知的这部分数据去掉
2. [ICLR] INSIDE: LLMs’ Internal States Retain the Power of Hallucination Detection
   1. LLM幻觉检测问题，提出了一种新的幻觉检测方案
   2. 已有工作主要从token-level和sentence-level对不确定性进行评估，或者生成多个答案来比较相似性，这种对解码后的语言句子进行事后语义测量，不如对逻辑一致性/分歧进行精确建模
   3. 本文提出使用llm内部隐层状态对幻觉进行检测，使用llm非任务头的最后一层表示得到句子嵌入，然后使用多个回答得到多个句子嵌入的协方差矩阵，最后分解求解协方差矩阵的特征值，对数平均得到EigenScore，分数越低说明越一致，也就是越不可能出现幻觉
   4. 进一步提出了一个测试时特征修剪算法，将特征值限制在特定区间内，减缓出现幻觉的可能
3. [ICLR] Towards Understanding Factual Knowledge Of Large Language Models
   1. 提出了一个新的数据集Pinocchio，用于测试LLM保存事实性知识和程度的范围
   2. 在数据集上研究LLMs是否可以组合多个事实，及时更新事实性知识，对多个事实进行推理，识别细微的事实差异，并抵抗对抗样本。
   3. 结果说明现有的LLMs仍然缺乏事实性知识，并且存在各种虚假的相关性。
   4. > 这个后面可以看一下实验怎么设计的，可能有用

# 幻觉检测方法
## 2023

1. [Arxiv] Complex Claim Verification with Evidence Retrieved in the Wild
   1. 事实性幻觉检测方法，论文中描述解决的问题是事实检查中的证据检索
   2. 已有方法要么从黄金证据的文档集合内检索，范围有限；要么执行无限制检索，可能检索到事实检查人员事后编写的文档，造成无效检索。本文从网络上检索证据，仅限于在声明发生之前撰写的文件，而不是来自事实核查网站的文件
   3. 首先将复杂声明分解为一系列（10个）yes/no的问题（使用text-davinci-003），然后使用商业搜索引擎搜索相关文稿（bing-web-search-api），在此步骤加入时间限制（避免检索到声明出现之后的页面）和站点限制（避免检索到事实检查网站），使用html2text和readability-lxml从url中提取真正的内容。随后对提取的内容进行二次检索，提取出最相关的文本块，使用`BM-25`算法检索最相关的K1个文本块，然后使用k2单词的上下文进行扩展，并合并重叠的文本块。最后使用大模型（text-davinci-003）根据提取的文本块进行总结。最终训练大模型（DeBERTa-large）来根据声明和总结的拼接输入判断声明是否真实（从真实到不真实一共六级）
   4. 这个东西海缆项目好像用得到
   5. 但总体来说幻觉检测方法就是根据得到的输出，和从网络上检索到的事实，完成一个分类任务
2. [Arixv] FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
   1. 生成式AI的结果的事实错误检测方法，目前存在(1)更广泛的任务在使用生成模型处理时面临越来越大的包含事实错误的风险。(2)生成的文本往往篇幅较长，对个别事实缺乏明确界定的粒度。(3)事实核查过程中可获得的显性证据稀缺等问题
   2. 提出了一个任务和领域无关的大模型生成文本的事实错误检测框架，但简单来说就是给不同的任务定义不同的声明，然后使用不同的工具进行检测
   3. 整个框架包括声明提取，查询生成，工具查询，证据收集和事实验证五个步骤。将涉及的内容分为prompt，回复，声明和证据四类，分别为五种不同的任务针对五个步骤设计了对应的方法，引入了对应的工具。
   4. 和上一篇的方法区别是对生成的文本首先进行了摘要理解，提取声明，然后对声明分解出问题，再拿去检索证据
3. [Arxiv] Retrieving Supporting Evidence for LLMs Generated Answers
   1. 事实性幻觉检测方法，通过检索外部数据源，使用大模型判断生成结果是否包含幻觉
   2. 主要贡献点是对于主题漂移的缓解。首先让大模型回答问题，然后将问题和大模型的回答整合为一个新的问题，使用（文中设计了三种）信息检索方法从外部数据源获取相关信息，最后将问题，大模型的回答，检索到的相关信息一起提问大模型，让大模型判断两个回答在问题的语境下是否一致。等于是用外部检索的数据作为准确回答，然后让大模型判断生成的回答是否正确。
   3. 主要结论是当提供了足够的外部数据时大模型可以识别生成内容中是否出现幻觉，但准确度只有70%-80%，无法当作一个可靠的幻觉检测方法。
   4. **但考虑到这个评判逻辑，是否应该调整prompt让大模型根据检索到的结果来判断，因为可能会有使用生成的幻觉回答作为正确答案的情况。以及检索得到的结果质量决定了检测的上限，那是否应该对检索结果的质量进行检查**
4. [EMNLP] FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation
   1. 大模型生成的长文本的事实检测方法，提出了一个新的指标，以及一个自动化的检测方式
   2. 主要问题是长文本中包含一系列陈述，无法用一个简单的标签来表示完全属实或不属实。已有方法要么添加一个部分属实的标签，但是主观定义可能导致低认同度，或者就是严格要求完全属实，忽视其中有部分属实的情况
   3. 本文定义事实分数，将长文本拆分成若干个原子事实，使用原子事实的属实比例作为长文本的事实分数，用生成的所有长文本的事实分数的期望作为模型的事实分数
   4. 本文首先手动度量了不同大模型的事实分数，然后发现所有模型的长文本的事实分数都很低，稀有实体相关的错误率更高，文本靠后位置生成的事实陈述错误率更高，可以进行检索的模型的事实分数反而很低，商用模型可能也没那么可靠。
   5. 然后本文搭建了一个自动化的评估流程，首先使用instructGPT将长文本拆分成若干原子事实，然后使用另一个大模型对不同prompt加持（纯事实/数据源检索/非参数概率/检索+非参数概率）的原子事实进行判断。发现检索可以有效降低评估模型的错误率，提升有效性。
5. [Arxiv] A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation
   1. 事实幻觉检测和缓解方法，通过概念相关的token的生成概率（logit）来判断是否产生幻觉
   2. 整个方法分成检测和缓解两部分。检测部分首先（使用实体提取/关键词提取/大模型）识别出每个概念相关的token，然后利用这些token生成时对应的logit（均值/最小值/平方根）来判断是否可能出现幻觉，最后使用问题生成工具/大模型生成用于验证的问题，并检索搜索引擎/大模型本身获得概念相关的知识。缓解部分首先让大模型回答验证问题并验证回答，如果回答正确就继续生成，如果不正确就中断生成，提示模型删除或替换幻觉信息来修改生成结果。
6. [Arxiv] Zero-Resource Hallucination Prevention for Large Language Models
   1. 事实幻觉检测方法，通过对概念的熟悉度分数来判断是否产生幻觉，避免使用外部数据源，产生之后避免回答问题。
   2. 方法依然遵循提取，识别范式。整个方法分为概念提取，概念猜测和聚合阶段。概念提取阶段使用命名实体识别模型提取概念实体，然后通过实体合并和过滤完成预处理。概念猜测阶段首先提示大模型对概念做出解释（贪婪解码），然后将概念名称遮罩，并再次提问大模型让其根据解释输出概念名称（beam search），概念名称的预测分数作为大模型对于概念的熟悉程度。最后在聚合阶段使用每个概念的出现频率计算概念的重要性分数，并使用重要性分数对概念的熟悉度进行聚合得到整个提示的熟悉程度，如果熟悉程度过低就会避免进行回答。
   3. **这个直接不回答有点难受，前面那篇文章可以看作一个扩展，对于不熟悉的检索外部数据源，可以用in-context提升熟悉程度。**
7. [Arxiv] SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models
   1. 事实幻觉检测方法，黑盒方法，使用模型生成多个回复，然后计算不同回复之间的一致性
   2. 首先使用模型根据用户查询输出用户回答，然后使用同样的查询生成N个随机回答，最后计算用户回答和随机回答之间的一致性，从而判断是否出现幻觉
   3. 为logit，句子相似度，问答任务，自然语言推理任务等等均设计了不同的一致性计算方法，包括使用概率，使用大模型，使用熵等等
   4. 逻辑也是如果模型知道对应的事实知识，那多次生成的结果应当是相似的。如果一个概念在多个回答中均出现，那这个概念可以被假定为事实，但如果只在一个回答中出现了，那更有可能是幻觉。
   5. **那如果大模型根据自己过时的知识一本正经地胡说八道怎么办**

## 2024

7. [ICLR] LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples
   1. 对于事实性幻觉的检测方法，提出了一个新的观点，即幻觉是一种新的视角下的对抗样本
   2. 提出了两种方法用于生成对抗prompt，使大模型输出同样的幻觉回答。基本方法是从wiki中构造具有事实依据的问题，并提问大模型得到事实正确的回答，然后从中随机替换一些主体得到事实错误的回答。然后使用基于梯度的token替换策略构造攻击prompt。
   3. 替换策略包括引入对抗token，然后在每轮训练中计算通过将第i个token与对抗token交换而产生的对数似然变化的一阶近似值来选择最好的k个对抗token。最后将对抗token换入prompt得到对抗prompt。
   4. 在幻觉检测上为生成的第一个token的熵设置阈值进行过滤，因为观察到在生成正常回答时第一个token的熵一般较低，但生成幻觉回答时第一个token的熵较高。
8. [ICLR] Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs
   1. 对于事实性幻觉的缓解效果检测方法，做了横向对比，将过程分为prompt工程，采样策略和聚合操作，从而尝试设计黑盒的幻觉缓解方法。
   2. 整体逻辑是通过可信度激发来让模型反馈对于生成结果的自信程度，然后使用这个自信程度来判断是否有幻觉，原来的方法一般需要对于模型的白盒接触，本文讨论了黑盒情况下仅使用模型输出检测的方法
   3. 整个自信度激发框架分为三步：首先是通过不同的prompt方法（简单提问，CoT，self-prob，multi-step，TopK等）提示模型使其输出回答和对应的置信程度，然后使用不同的采样策略（多次输入相同的提示，换一个方式提问，故意在提问中添加混淆项）获取对同一个问题的不同回答，最后使用不同的聚合策略（简单相似数平均，使用置信度对相似数加权，使用先后偏好对概率加权）聚合回答得到不同回答的一致性程度，作为整个回答的置信度。
   4. 进行了置信度和错误预测实验，得到的结论包括模型的输出很容易过分自信，置信度和错误预测的表现会随着模型规模增大提升，原始prompt的输出表现出明显的过度自信和糟糕的错误预测效果，人工设计的prompt可以部分解决幻觉问题，但没有一个鹤立鸡群的方法。
   5. 多个响应之间的差异改进了错误预测，多个回应之间的一致性对于错误预测的提升效果比单一回答的置信度分数重要。
9. [Arxiv] Do Language Models Know When They’re Hallucinating References?
   1.  事实幻觉检测方法，黑盒检测方式，通过直接查询和间接查询的方式来检测一致性，从而判断是否发生幻觉
   2.  基本逻辑依然是如果是大模型知道的事实，那应该无论怎么问都能得到相同的回答
   3.  设计了三种直接询问的prompt模板，每次生成多个回答，然后用其中模型回答yes的占比作为真实性分数。
   4.  设计了间接查询模板，间接查询标题对应文本的作者（有点类似侧信道），然后再用一个prompt查询大模型来询问间接查询结果之间的重叠程度作为真实性分数
   5.  **但是没明白它所说的hallucination reference是什么，感觉像是探测训练文本是不是存在**
10. [Arxiv] LM vs LM: Detecting Factual Errors via Cross Examination
    1.  事实幻觉检测方法，使用多个LM交叉验证来检测幻觉
    2.  逻辑依然是幻觉容易导致不一致性，而没有幻觉的结果
    3.  框架包含两个大模型，一个大模型作为检察官，另一个模型是受试者。根据受试者生成的声明，检察官生成问题来试图让受试者生成与原始声明不一致的回答。
    4.  第一步是初始化，设定模型角色，向检察官提供初始声明，要求生成问题。然后向受试者提供问题，并拼接prompt使其根据声明生成回答。第二步是后续问题，向检察官模型提供受试者模型生成的回答，然后提问检察官是否有后续问题。如果有的话就让检察官继续生成问题，让受试者接着回答，直到没有新问题可问。第三步是事实性检测，最后提示检察官总结声明是否是对的

# 幻觉处理方法
## 2024

1. [Arxiv] Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation
   1. 对于事实性幻觉的处理方法，提出了一个新的数据标注方法，以及一个新的基于事实反馈的强化学习框架（RLKF）
   2. 首先评估了LLM识别并表达其内在的知识状态的能力，使用实验说明了LLM自身可以识别是否含有问题所需事实性知识，但在生成过程中不一定会按照知识来生成。
      1. 外部视角使用多次生成的一致性来判断是否具有知识，内部视角使用一个MLP（探针）在问题最后一个token的多个层的表示上预测来判断
      2. LLMs在内部知识状态方面具有强大的自我意识，通过知识探测展示了超过85％的准确度。
   3. 然后提出了一个标注方法，对同一个问题生成多个回答，然后计算和正确回答的相似性，重叠程度，以及探针对问题表征的预测分数，最后将这些分数归一加总得到一个真实性得分，根据中位数分为正确回答和错误回答，五个答案全都正确视为知道对应知识，全都错误视为不知道，其他的视为混合。
   4. 最后提出了一个框架RLKF，利用真实性评分得到的偏好数据训练奖励模型，然后利用奖励模型进行RLHF训练，让得到的policy模型遵循事实性。
   5. > 这个扩展改写成DPO都没什么创新性可言，但是这个“LLM自己能够意识到自身是否具备问题所需知识”这个结论还是挺有意思的，以及生成时不一定按照所具备的知识生成这点，如果这样的话RAG岂不是意义也不大，就算查到了也不一定会按照查到的知识生成结果。
